{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gmbRYKD1odtB"
   },
   "source": [
    "# Tarea 1: Perceptrón Multicapa\n",
    "### EL4106 Inteligencia Computacional\n",
    "\n",
    "Profesor de Cátedra: Pablo Estévez<br>\n",
    "Profesor Auxiliar: Ignacio Reyes<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gr3dCq8pYd_w"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Wiv1sT9tpZO-",
    "outputId": "a0bd7f05-2f40-44aa-bd7c-90a26898e458"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.6.0\n",
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import itertools\n",
    "import warnings\n",
    "from sklearn.metrics import confusion_matrix as sk_conf_mat\n",
    "\n",
    "# Se imprime la version de Tensorflow\n",
    "print('Tensorflow version', tf.__version__)\n",
    "# Se imprime si es que se esta utilizando GPU o no\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    warnings.warn('GPU device not found')\n",
    "else:\n",
    "    print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lAqEQZkkRgTJ"
   },
   "source": [
    "## Preparación de la base de datos MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwD-4ePxRuzZ"
   },
   "outputs": [],
   "source": [
    "# ----- Digito a identificar.\n",
    "chosen_digit = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RahVklIFRfbE",
    "outputId": "962ab9ec-8076-43d1-a599-6ae3bcc18c24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ready.\n"
     ]
    }
   ],
   "source": [
    "def process_dataset(images, labels, selected_class):\n",
    "    \"\"\"Revuelve datos y selecciona subconjunto segun la clase seleccionada.\n",
    "    Aplana las imagenes para que tengan la forma de un vector de largo 28*28\n",
    "    Normaliza imagenes para que en vez de tener valores entre [0,255] tengan \n",
    "    valores entre [0,1].\n",
    "    \n",
    "    Los datos de imagenes y etiquetas son revueltos, se seleccionan aquellos\n",
    "    que coinciden con la etiqueta de la clase seleccionada, y un subconjunto\n",
    "    del mismo tamaño que el anterior es seleccionado de entre todas las demas\n",
    "    clases para obtener un problema balanceado.\n",
    "    \"\"\"\n",
    "    # labels a formato one-hot\n",
    "    aux_labels = np.zeros((labels.size, 10))\n",
    "    aux_labels[np.arange(labels.size),labels] = 1\n",
    "    labels = aux_labels\n",
    "    # aplana imagenes\n",
    "    images = np.reshape(\n",
    "        images, [images.shape[0], images.shape[1]*images.shape[2]])\n",
    "    shuffled_indexes = np.random.permutation(len(labels))  # Primer shuffle\n",
    "    images = images[shuffled_indexes]\n",
    "    labels = labels[shuffled_indexes]\n",
    "    selected_column = labels[:, selected_class]\n",
    "    selected_images_indexes = np.where(selected_column == 1)[0]\n",
    "    selected_size = len(selected_images_indexes)\n",
    "    non_selected_indexes_subset = np.where(selected_column == 0)[0][:selected_size]\n",
    "    indexes = np.concatenate(\n",
    "        (selected_images_indexes, \n",
    "        non_selected_indexes_subset),\n",
    "        axis=0)\n",
    "    # No queremos que el modelo primero vea todos los datos de una clase y\n",
    "    # despues todos los de la otra, asi que volvemos a revolver.\n",
    "    np.random.shuffle(indexes)\n",
    "    images_subset = images[indexes]\n",
    "    labels_subset = selected_column[indexes]\n",
    "    labels_subset = np.array(labels_subset, dtype=np.int32)\n",
    "    return images_subset/255.0, labels_subset\n",
    "\n",
    "\n",
    "# Carga de la base de datos MNIST\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reserva 5,000 muestras para validacion\n",
    "x_val = x_train[-5000:]\n",
    "y_val = y_train[-5000:]\n",
    "x_train = x_train[:-5000]\n",
    "y_train = y_train[:-5000]\n",
    "\n",
    "\n",
    "# Preprocesamiento de datos de entrenamiento, validacion y testing\n",
    "training_images, training_labels = process_dataset(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    chosen_digit)\n",
    "validation_images, validation_labels = process_dataset(\n",
    "    x_val,\n",
    "    y_val,\n",
    "    chosen_digit)\n",
    "testing_images, testing_labels = process_dataset(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    chosen_digit)\n",
    "print('Processing ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "RLcbP5NJRyH_",
    "outputId": "22fc9be1-b92c-45b5-c2c0-a362e28af55d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAABuCAYAAAAj1slPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABogUlEQVR4nO39d7Rl133fCX72PvncfO/LoV7lgBxIkGAQKQgURUuyJMuShpYVOLY1clhte7xGHrdXt9vL7mWPRy212u7VljWy3FLbiqZtRYpRFANIggABFFCFylUvp5vjSXvPH+dVoQoAQYBA4b2qOp+17qp675177rn77rvPb//C9ye01pqMjIyMjIyMjNsYudsXkJGRkZGRkZFxs8kMnoyMjIyMjIzbnszgycjIyMjIyLjtyQyejIyMjIyMjNuezODJyMjIyMjIuO3JDJ6MjIyMjIyM25631eD52Z/9Wf7ZP/tnb+dLZlxHNv67Rzb2u0s2/rtLNv67Rzb216HfBAsLC9p1XZ3L5a49/vbf/ttaa61/7dd+Tb/3ve99M6f/tvmpn/op/Y//8T9+y8538uRJ/d3f/d26VqvpNzlkbyl3yvhrrfUv/MIv6MnJSV0sFvXHPvYxPRqN3tLzv1Gysd9d7pTxz9aeN8adMP/vlLG/GXP/TXt4/uAP/oBer3ft8W/+zb9500bYXsOyLH70R3+UX/3VX93tS3kFd8L4/+mf/in/8l/+Sz7zmc9w+fJlLl68yD/5J/9kty8rG/td5k4Y/2zt2V326vy/E8b+psz9N2MtLSws6E996lOv+P2pU6e04zhaSqlzuZwulUpa61dagP/qX/0rPTU1paenp/Wv/uqvakCfO3dOa631Bz7wAf0rv/Ir1459ueV6+vRp/fjjj+tKpaKPHj2qf/u3f1trrfUv//Iva9M0tWVZOpfL6e/7vu/TWmv9L/7Fv9AHDx7U+XxenzhxQn/84x9/w+/33Llze26XdSeM/0c/+lH9j/7RP7r286c//Wk9OTn5up9/M8jGfne5U8b/Ktnak83/q9wpY3+Vt3Lu35QcnhMnTvBv/+2/5dFHH6XX69FqtV5xzCc+8Ql+/ud/nk996lOcO3eOT3/606/7/P1+nw996EP8lb/yV9jc3OQ3f/M3+Vt/62/xwgsv8DM/8zP8+I//OD/3cz9Hr9fjD/7gDwA4dOgQX/jCF2i32/yTf/JP+Kt/9a+ytrYGwOLiIuVymcXFxbfk/e82t9v4v/DCC9x///3Xfr7//vvZ2NigXq+/gVF5e8jGfne53cb/VuN2G/9baf7fbmN/M3jTBs8P/uAPUi6Xrz1+5Vd+5XU973d+53f42Mc+xj333EMul+N/+p/+p9f9mn/4h3/I/v37+djHPoZpmjz00EP88A//ML/3e7/3TZ/zIz/yI8zMzCCl5Md+7Mc4cuQIX/va1wDYt28frVaLffv2ve5r2CvcCePf6/UolUrXfr76/263+7qv+WaQjf3ucieM/17mThj/vTr/74SxvxmYb/YE//W//lcef/zxN/y81dVVHn744Ws/LywsvO7nXrlyha9+9auUy+Vrv4vjmJ/4iZ/4ps/59V//dX7hF36By5cvA+lE3t7efsPXvde4E8Y/n8/T6XSu/Xz1/4VC4XVf880gG/vd5U4Y/73MnTD+e3X+3wljfzN40wbPN0MI8Zp/n56eZmlp6drPL3dr5XI5BoPBtZ/X19ev/X9+fp4PfOADfOpTn3pdr33lyhX+xt/4G3zmM5/h0UcfxTAMHnjgAfRt3Cj+dhr/u+++m2effZYf/dEfBeDZZ59lcnKSWq32up7/dpON/e5yO43/rcjtNP632vy/ncb+ZnDTdHgmJydZXl4mDMNX/fuP/uiP8h/+w3/g1KlTDAYD/uk//ac3/P2BBx7g4x//OIPBgPPnz9+Qqf193/d9nD17lt/4jd8giiKiKOLJJ5/k9OnT11774sWL147v9/sIIRgfHwfg137t13j++edf93vRWjMaja69l9FoRBAEr/v5u8HtNP4/+ZM/ya/+6q9y6tQpms0m//yf/3N++qd/+nU//+0mG/vd5XYa/2ztyeb/G+F2GvubMffftMHz/d///eTz+WuPH/qhHwLgscce4+6772ZqaoqxsbFXPO8jH/kIf+/v/T0ee+wxDh8+zGOPPXbD3//+3//72LbN5OQkP/VTP8WP//iPX/tboVDgk5/8JL/1W7/FzMwMU1NT/MN/+A+vDcZf+2t/jVOnTlEul/nBH/xB7rrrLv7BP/gHPProo0xOTnLy5Ene+973Xjvf4uIi+Xz+myZPXblyBc/zuPvuuwHwPI9jx469uYF7i7gTxv97vud7+Lmf+zm+8zu/k4WFBRYWFl7xRd0NsrHfXe6E8c/Wnmz+vxp3wtjfjLkv9B7yrQohOHfuHIcPH97tS7kjycZ/98jGfnfJxn93ycZ/97iTxj7rpZWRkZGRkZFx25MZPBkZGRkZGRm3PXsqpJWRkZGRkZGRcTPIPDwZGRkZGRkZtz2ZwZORkZGRkZFx2/OawoMfkj/ydl3Hbcmn1O++qedn4//myMZ/d3kz45+N/Zsjm/u7Szb3d4/XGvvMw5ORkZGRkZFx25MZPBkZGRkZGRm3PZnBk5GRkZGRkXHbkxk8GRkZGRkZGbc9mcGTkZGRkZGRcduTGTwZGRkZGRkZtz2vWZaekZGRkZGxpxACYVoIQ4KUYBgIIdBJgo5jUBqdJKCS3b7SjD1GZvBkZGRkZNwymJMTDO6fZ1Q1CUqC4ZQgcTTehqB4JcEcJHiLbdT5K+go3O3LzdhDZAZPRkZGRsYtg5qosPGIzXAhZGauwc8d+BxH7A3+l9UP89WvHsNp2oxbFfzF1czgybiBzODJyMi4cxACYRggJMJ1EL6HkDuhkasoBVqjtYY4hihGa40eBeg4gqzf8u4gBAiJdiyigiZXHXKg2OB+Z4XDlsO83+QrjkaZoI3dvtiMvUhm8GRkZNwxyHweOVZFew7tuytsPyCJcxptpg80yKHECAQyFLjb4LQ0Tjchf6qOXl5Lc0XCMDN83k6kgcz5CMemP+lhH+rwgwef44S3Ss1IP4ftII+9beBtgd2JIclyeDJuJDN4MjIy7hiE7xFNl4kKFpsPSz72Fz7Lu3LnqckBY0aEAk6GY5wLplgJKvzp4nE2Vwu4myZ2u4RdbyKiGHU1QTbjbUEYqcGD7zGsGTwyu8jPVJ/AF4KSdFEo6kEOpwluXWF2w9RDl5FxHXvb4NlxYQrDSDPyDQPhOmCaqRvaMMCQIATaMneOF9d+B7y0C0sUJAkiUTAcoYMQkgQ1GGQLV0bGnYAQCM8lqDoEZUlcTlhwtpkxuuSkIickEZqa7NM12wBMF7qMKjZBLBjVLOyJGmIYIJIEnSRpeOxalZACrdBKg1aZB+gtRNgWuloiLnsEZUHV6uMLgQI2kiEDLdge5jAHGmugEGGcfg4ZGdexZw0eYdkIy0TYFqJURPsuKu/SXfAZlSSJI4iKoCyIPU1cUmhLIZwEy40xDIXWqdGjlCDs2YiBgTGQFC5Bfi3Bbsc4Z1aJ19Z3+d1mZGTcNK7m7RgG4UKNtfcZRJMh9x9c5ri9RlkqNhKLZ+IKColEUTCGuDLih2eepj3p8VR7gSeTo/Smx/G2FZWnLOTKOsL3oVwA00COQnRvAHGMHg5Ro9Fuv/PbBlmtsPFojc4hUAtDHspfwRcWz0eCT3bvZTUos3p+nIPnAuz1LjTaqCyklfEy9qbBI0Rq7DgOwrFR5TxRxSOoWDSPGgynErSfUBjrU3YD5got3lu+wKTVYr+1zTErxhXpW0u0JtAxTwYlnh3t4/xggk+W7yHOWbh1g7G1PKzt8vvNyMi4qQjTBMNgOG7jHW/x+OwlHilcZN6M8IXJmcjj+eE8CsE+e5t9VgPf6PIOZ52qtPm6f4H//kSZFb9GuGRRvJTDqDuIQo5orIByDMxugAEQRqmHJwgyL89bhC74tI/A5AMbHCtvctxewxIGW0mOL24fYrVTxF82cC6vo9Y2dvR4MoMn40Z23+C5uvsCpO+D5yIsCzVWIqy4JI7BqGoQFgRhQTCcTTCrI1wvZK7UpmwPmfebzFhNakaPsgxxhY0jLAASFIYQ1Iw+M1aLnuNiFCLCooUR7ITCMm5ECIxSEZHLgWmgHRttWzceY0qUa6INiYgSZBAjYoU2BEiJFiI9xrouvPhaKI0MY0SiEb0hemMbPQrSEMHVnVp288j4NpD5PGJ6ApVz6U8aTBR6zDlNXBFRTwRdEfNiMM1z3VkA/FLIfnsbA40CIhIMFGNen61ynqBvMJx0yQ0mCKs+vTmb2BWYQxu7l0NGGqsTYbZGiCRBdPqoThd2kp2zEPrrR1g2wpAo3ybJKWbybSadDq5IAINWkmOjW6Db9Cn3gDhBJyoLZ73V7KSXSNdBVsrg2GjLRPkOmBJtCLRM13kZxMhhBHEC9SZJo7ln1u5dv9sL20Z6Llg2amGS/r4cYV7SuBvco208O2Ii12Pc7ZEzQha8bapGH1dG1IwerojwZcC4DHAF+NLATPdZJFoBIJEsmCPK8gqzZpOTczOcZJau7zFWdrP+Gi9DOg7xXftpH/aJfBhMC8LqjbslnUuojneo+kPWOgV6GwXkSKJsBY5CGBqvMKKW72EZ33qn1Q0cGu0cycjEP1dm/pM+cnkLwgg9HKaLWKaemvFtIGYmWf3wBP1ZjXGgy4/MPMXD7mXqSY6vjvbTSnx+b+khVs+Now2Nuk/wvtwZXJHQVQbddBnhQ+OnuKe0yhO1AyyNZvEXqgxmNLkTTSYLXSJlEMQmoZIsNQqwVcYYCiqnx6ic6iCHEWJjm6Te2N0BuUUQpomslqGQoz/rk5vr8gPjzzBlthg30g/l9HCG3pkKxVVB6VKE7vUz6YC3GiEQto0wTcS+GbbfOcZwXBBUNNFciO2HeE5IyRshhGZpswqrRayeYPqJCvbnntszeki7b/AYBjgOwnUYjXt05g3CEkzcv87/cPiPqBo99pshNelde47i5ZNZAt7O/1Ir86qxc5WSdClJcESHe0urNAOfy0OTxDUyg+flGAajCYfOAUGU13iHWzw0sYYUL437kdwmHy0/yVErx1dGCb9efy+rgxI1p8+c1yRvjHjIu8y7nP41b9trsRwP+fzwIMthlV+zHiV42sOvu2lCehimn6pWvOxjzcj4liQlj87hhMkj2zw8tsz7vfMctCy+HgRcCsZZD4qsbZXILRpoA1YPljDQSDQDbTLSJgmSe90l7nWXsETCbyzU6LoO3v4uf/fYZ3nAWbr2/Yi05LP9E3yhfoT1XoHucBx/08Pqmlhtd5dH4xZCSITnkhR8woJkptjhne4ivtD4OykLW2Eeb0NQWEpwtkboLIz41iNkauzYNnHZp7NfMJqLqEx1+OjBr3PMWWOf2eSEnd5J/8+5BX536mFWmiX6lwo4hkRHu/wedtgVg0dYV706Jnp6gv7+IrEv6eyT9BcSdD7maHmLshyQEzF9pQn1gEDDSpKnkeRJtCTSBgmSbuKyEZUIVLowXcUSCa6M8GXIO/yLPGSnSYQTVof9hQar+RLKdnZjCPY0wjAI85JgLP0sZoodjuQ3bzhmzm7g7izwBRlywNsiZwRUzAGTVhtXRpTlEPk6zUlXwLjZAWB+ssnWgzP4U7PkNmK8C3XEYITu9ki63WxBy3hdXM3biX0LXYiZL7SYcVrYQpFoTVd5rI7KrA2L6LaNW9doASuLNX6p9DiWUGwFeTrBjUbKWrOIecHFaQp6FPiPpXfxteJBFtw693uLWCKmIEcczG+TNwOe2FemPrCxehZldxrXsSEIUc0WajDYpdHZ+wjLJKkUGM34jKqS/e4AV+idyqyYhISlfgW7rXEbEUYvyBKV3wgyjYRcq4KWMhXdBLAsZLmEzntoxyIY84jzBr0pg9FsRH68T8UfshkWUVqybpZZSRoYaHqJy75cE6UF27UicnoS0Rugu91dT+TfFYNH5nPo2UlU3mbzHXn67+1TK/d4R2WTdxQvUzYGHLfXWDAjIq25EnusxBWWoiqf2LiblXaJJJGEoYlOBLpp468amANAg0gAAYkDiQtRXnPXey/yi/v/M5aAd3oX2W9v0Q5dNnIHyUyel2Fb9Gckh+5aYtpv81jlRR5yFzGu86z5IqEqTRKtmDPhBwrPMdIGFgpXKAwBvhAYwn5dL1mQJvfb20TWNvOH6nxx8hhbYYE/euY+Zv90HG8rxFpxEJmMQMbrQJgmMp8Dx2FUs5if2eQvT3ydWbNJaSfXYDGqcbI+TaOdI3fZoPp8FxEpnE6eZ0/fg0jA31S4jbQh5VX2RQqz1UKMIkbzJbYuz7NSnCc4NuSj936dA84WM1aTh7zLRNrgwdIiJ++ZZbFbZempaSqnp3HaCYVnTdTlxd0aoj2P8Fy6B/Npocpswj2FVarSZCOJeXI0z3pc4uzqJAfOjbBOLaNHo1QQMuNbIkzz2oZAeC7C80CKdJ5rjc55tO+t0Z03CIsQHhkyPdZkxh3yXcV1xqweK0GZk60ZBpGN0oJECwyheWhsiR+oPc16qcy/ODJF9/7JtCL6/CZqaXlX3/fuhLRsi6ToEBUsBlOaxw+f4YH8Ivc5S9xnJ1jCANJHTwV0lMtqVOHicJwr9SrBpg+xwBgKjATcTUHlbITdiRCJQsQKhCDOW0R5k1HF4MKJGoMFg5JImDLSnJ8Zr8Oa9ToSau80hCT24URpnf1unXudZY5ZN2q1S1JDRqHxhc0BExQKeKWme/r718YSBuM7yesz5oj3uc8y0jFnOhM0K3MYIwvTdUBkAciM14GQYNkI1yF2BftyHY7bGxTESxWcvcSlM3CJejbFtsbcbMMooAA4bQ8ZKpwLm8SvskhfndHeYI6KMUVYMtksumwcLTJmdTkkNzlmxVhopoxneY9/jpOlef6/G9/NoOmhTIO8l221XhPTJCxIgjGFKIeMmd2d8HjMelxiaVQl6VqY9Q7J1tZuX+2thZBgpR3nheOgPSdNTN5pq6IKLoNxSX9WkVRjHjt8jg9XT2KLBF8E2CLhy/oITwb7aPc9otAkCQ2EoeiWtzhibTNhdDHKIcOqh5YCew/M990xeEoFOgc8RlVJOB1y2Ntkv7WFLyN6OiFRIU8GEzw/nGczKvCVjf1s1QuovoW3YlJspV4cI9SIBNxWgrs1RPaD1ELdcctZgY0xtBGxw2bT52Qww5TZZtzoU5UJjozQmb3zSuIYdxs+u3iU8UIPayahIE/d4OEBbghWvd7UGgkUpMAVqXFjiVcaSFfikCdH+9iISlxYHmd2W+HUA2Rv8IrcrIyM67m6azWqFQb3zzOYMGkdg8fzmxREjCGgpWIiDS/0ZgiW87gNiVdPYBSgRyOMdh8bEFGCHg5f8/X0cIizPcQc2BQvuHy6eDefKx7hHQuL/PTkl6gaPWwsyjJg3q5zcGab8+EkQdmisFjE3apBGGUCqNdzVXDWdRhMCbyDbWZLbebtOpCmNXx2+zgX6zXcNRMRZF6dN4oxM0lwcJzYMxjWTIbjAi1BRiBjiD3oHY4pT3couAGeEXElHGNpVOXp7XnaQ5dey8NaszGHAjtOn6dM+Hp+nlPVKSJtIIUmLAhEIsnbVvrZ7mJKwq4YPNFkke0HgZkB71pY5LHcaebMmIHW1BPJRlLgXy9+FxdOzWB1JJVTmsMXBoggRPZHL03wZKfJXxil7syr8durcUjDwDAMcmNVnOVpPtW8m31eg8cLz3PAjMmbwbVSuoyX0GFI6UJE3SuxUinyXx/RjM93kOIlY+Plxg9AwrceS1dE7LcaTBkvZVu93Oh5crSP/+XM47SaOQrPuBRe3IKtBmowfOkzzsh4OUIgPA/h2ET7J1n+TpPKPds8XN7mu4snGTdMWipmNXFoJT5fX5+n9ozA34jwFzuoVhsVRshRgNiyQClUELzmS6pWGxGEWIZkaqVC7bkScc7k6e86hv3BmAN+nffmzvKQ06Ikt/l7C5/i4tQkf7RxDxsrC0zWp9I1bWU9M3h2uCoSqQo5+odDfv7uP6Rm9DhsdVA4nBrt4/nnF8hfNChfSaDT2+1LvrUQgtGhCZa/yyasJswc2OAn5p6lIEcsh9U0EdyIeCC3yBF7nXqS5yu9wzzbmefJpX0UPplj8kLAzCjCaHcgSuetSBTad7lk1/jc3AlyZoA0FKMJjbIEScF5HXeIm8vbb/AIQeIZJJWY2WqHg/42Y0ZEQToMkoCutthMCqy2inhrBnYLShf6yOfOo6OY5NsoOTQcB2MoqAc5CtaIkbYwRIJ8lZt2BpAk2J0Ib8tEKMFWL8dqVMESNxob1xtASr92qEkKhYHClwEDo0soY+xX0edJtGYrLtBq5ZBbNk5TI1pdknY3NXayhOWMb4aQqYveskg8k3gi5P1TF9jv1pky+ljCBmK6yqWR5BkMHErN1DssegOSMAKVoEYJvM7kSh3H6G43ffnhCLPZxsr5ePcusNSrYAlF1/NwRRdXwF3WNrNGm8VylaXiAkneRih1TYssg2tVQdoxsAsh73RX8YXAEgaJ1jTiPFZL4m1qnGacdrTPeP0ISZwzCMdicuMDPjB5nr9eOkleOlyKz7IaF7BEzII5YNLwuBRv8SWO0gh8wrZD6VKI/ewldBiSDEc3SIXIXA67XWM7zKEQCAGxDcoGbYg7x+CRvo+cGEP7Ltv7Lebn1vmOyfPsc+pcjPNciRW/2/wgn1s6Qr/v4rzgUTmTYPUSjO0uKop5ozXJwnGQjgOlPFFJc1dxjVmnyUhbPBuOuDysYUTZDfTl6DjG3OxQMgVe3aJhVPg3ix/i252t2tC440Pmqi0qzoAP116gJC/iCoVjaCygrUIuRi4t5fPFxmHMFQdvQ+Btx6lYW5K84c8/485CSIFwXXTeJyoa5Ms9HspfYdzoUBAaieT5sMa/W/0OlrtljPMe3kYfo95F9/pvfn4lSVoWLQXl8zHrX5plsTzNxgMFavN/SkGO8IWkJCMOuZsMFmK2+h7+hkOl0YVO560ZiFsYYZrI/XNEMyW68w7jpU1cIXaKVwxayuPrrX34K4Li5SFWY5AKlGa8cQQIceP9rywhMXoMtMmXR7OsRhVe6M3y6RdO4CxbVNbA2Wigg+DGNXlHQFjYNsoEz4jIGQF5L2C74BHFBsrefQmYt8/gKRYYHh5nVLNoH9X8P/Z9le/PneVi7HNyNM9aVOa/Pf0gU581qLQTvOUGYmUTohgVBG9cuEgIpOciyiXiap6oFvNdxRfIiZDL0RhnRjOcbY1jBJnB83J0HKMWVzDXt7AMSf7pHDivr9rq1VC+y9ajFS7enUOXI8acPnc7KygZUtAKV8BWIvls7y6ujKo8szRH5Szk1yLctR76ZbuIjIxXxTDAc1Eln1HJ4FC1zof9RSwhcUWaMPlE/zDPPXkIb00ycT7GuLBG0mq/Jd5DHcfpeUYBua9c4sCZIvFYntMs8Ony3czZDd7tXWTBNHnIu8y9JxZ5sTTJ8IJP6cUCLL0Vg3BrIxyHweEa9XssRuOaD9VWyQuLbR3y1Gg/ZwZTPL8yw9zZAPOZ8+go+pZhx4xXoiUgNVLoa/pREkFFupSk4koc8qnmPXx5eT+DjRyznxUUn91AjEJUvYG6mtu28525auwIx0ZZUDBHVMwB47ke3apDoDwSV+668N/b9/qGQeIaxJ5AeYpZq8G0mWdLjYi0ySBJm3u6jRinESDrHeJm+83d6Cwb7Tkoz0Q4CWU53JEkh5GySNSOSow0su7GL0MHQbpbBWi1v72T7Fj9slDA7paRgSAOJUGy0+eMtNtxojV9bbIeFlnqV0i6FnZfY3UjxDDMtDW+GTvJnZB6N17ODfL6d8j81oZEmxJtQN4KKMkbNXR6sYPVlTgtjdVJ0ty/t1IFVmvQSZrbMxhghTXMrk8jylEwRoRILCHJiZBpr81mKc9W3kNbWUjrag/FKC8JKpq4FFO1+gBEGjajIkvDClHPxuyHqJ1Q4rc6Z/rva/gW7pDvxvUIBSSCREnUdZU7EoHEwEDTjR1GAxuza+DWA/TKOmqnPcorxmunvB3HRhvgyBhLxjeI1e4Fds3gSnZyPqoy5n7vCpNWi0/sO0H9njJO06JiSoxOF6IItRNbfyMIw0DPjtM6UWRUk0yOb1CVIa6AI/YGVaPHi5VJTk5PUDi8HzEMUFvbuy6MdMtzXWdqY3yMZLxMWHZonJBU7tliItfjXaWLTBpDFLAaW4SkHY//6MV7EGsupSVBfrGHudlBd/voKIvRvxzpusixGtq1wbZQnpUKh11F7/QmG4aIOEG3OiTN5u5d8C6gdrEEUycJhEAUYw4El3tVAFqej2KAKxIW3AbtksdGoYyyjF3Pb9hNZC6HLOTR1RLNowZTD68xk29zn7/IQEecDKf4v158J+p8nvKywNyu81p3hGvpDIaB8L207Prlx+xozjAKSBrNlzZ4tztaYXVj3FWXQZDn7PgEo5qieN0hjoAFv8GVsQrrI5PYN7BtCyKRrsf6xtE3ZqfpHxsnLBkM94fc7a/giogrzQri+QKlFrhb3V3Pmt1tDxOThkdVhrTNFd4/d5E/ufseBk0Tu+dRvpxDjwKE1ujgDe7yDYPBXJ76vYKoGvORsRXKUuJLi5LUKKvLxdIFvjp9N4NDVexWiDkYvO5kxYxX55pr07aIZ6p0DucYlSXiri5/59DnmDLbHLKaTBo2bRXyYlJiKazx+a0j5L7hUT4f424FGOeWSdqd1EuRhbNegfA8kukqYckm8QyCkiS5TlNKaLB7CqubYAQKR2tote64naxCXVP7fj16UG/dCydolXqQzAGsdwtIoWmVfGCAI+Cwuw7As8VZtG3euQaPEMhCHjVRIZjIERwf8vNHf4eaTA2QtoJvDBYwny4w8/k+RjeAje3XPKV0HESxAJZJUisQlZwbJEiEvmrwgNkOkMMhyZ1i8ABWa0Ru2cHsG1w5WOXlnR9cITjkbrJdyadaVblCqmsFiEDcmO4mBOFMma0HLMKK5vCBDd7pXqGlHHrbOfZ9I8ZuRRhrDeJdXn/ePoNHKYRKdXNIINIm0Y6VaAkDXxpM2h284oihcgmKFrpaQgxGSEhdaW/w5qdsQZzTyFxEwRwhhUAiU0FJRFp1JEkfd+xq8xYgDaTr7JQFu4icj7ZMgjGXYU0SlqBa6DNrNSnLIQmChgrZTiwuh+NcHo2x2c1jdzR2O8boBehRcOuX6QqBMFNxLwwDYaUaMa91/DXX+/Vu9qu/lwIhUwl4XS4wnPQIC5LYE4QFgbJAJqlGlVCgd86VuBKzVsAcTaAT9dK5hbimW4NSaKV2lFZVWvmidJqXEobpd+8WcP1fFR4VCQxim64KsYTEwsAQAlMqEluT2AJlS4RtvTUh7avaMTufr7BSJVtRyKMccAyFLeNrcg4GYO+0vpFSoV+lYvGOYMcjrPM+Uc0nqFr4+Q4zRkBOSK7EBptJnkuDGlYHzEYfMQpQr6KoLBwnbVlkGIhSkaSSQ1sGQcUhLBmvYvAAWuPYEq9VxgB0FKPuhJxBpTCiVHcnUS8NjEKjdrYGORlQtfvk3JAwL2CsjBgGiDB6Rc6bsg3ivCYuJhSsEZGW9JWDCCRWN8Loheho9xtqvW0Gjw5DnEaAUGC1TVajCtvJKq5IvS4WBh/Iv4hxVLE0qvBp5ziD6XSSjz0/hnt2I114253XFXYSQhAUJNZUn8lylwPOFgYChaKrQkZasxGVMAYCqxNjDCLIckW+LczpSYKjU0R5k8G4QX9akHiacCpianaDGXfIRyZeYL/Zpq9N/rB7Lyd7s6wNipxdnoSmjbcuGbsYYK91EP1hWiJ8iyN9H7F/jqjiERUsejMmsf9NbmwCkp3yTQAZghGCFml7FGWmf4tKCuUqcBK8wgDHirHMhKodIoVmvV1gsJVDhAIZCmQkEQrkgyVEXEIoMIL0/IkHw3FNUkgQocBqS4yRwByB3dKYgcZfj/DObaIHI/RgkPZ+2qNGj45idKeLESf44x4vbkzw8YkjTJktHnI2GTcc9jl1kn0jOp6DUCbu8gSm56F7PZJ2542/N5kassIwEIU8lFKvQjCZZzhuERYFg3uH/OW5F5i2WxyxN3k1NfI7EWHZyJyHcF2aD0+y+QgklZgf2HeWnJCsJ/AvVz/Ck5cWEGsu8xdCqDdTI/zqZui6ELo4cYjt+0tEOcFoDIKJBG0pjHyMn+vdsKfVgFISrWHQ8MmfmcVtzJBfjfGeupx2tL8FDPy3EoVmOxmypUwibTFv1RkvdiiZQ/79B99Dd2Ecf10z+QUfeWUlTTcJAhCSoGKS7B8xU2vjGjH/rfMgq0EJb9XAXtlC9Aao7u7rJb2NBk+E0ejjRAl2p8RaWGJLmZRljE/q5XnUDXiH8wwtFXPQ2+bTM8dZbpRphnkmGiXkIEAMR68v7CQlUV6wf6zBsdIG83YdiSTRmoHWNBKLepTDHILRCxGDIMsV+TZRtSL1u1yCKgwXQh48eoUZr8MjhQt80L+8o6GRVsqcj2L+vH6EZy/MIzsmpbOS/HqC3Q5wL2yhGy1UHKPjW9/gEb7PYKFIb8ZkNCboHwsoVl69WaQUmoncgEkvTcRcGxRpDTwMqZjN9xhze0w6Xb639Cx32V0kcDXrQ+54B5TWfLx3kP9r+V10RmnOgtYCQ2pqfp8xt0+sJau9Eu2hy0yhy8fmvsRj/jIXI5c/7DzA4rDC5U6NpdUqYmBSOOsw3S4jmz2EVrCXm12qBNXuQK+Pu1ki3Mzz2cZxDue2OGTVGTdg3qpzZGaT1XyRfq9MOJnDNneCXp3eK3ITvhVCijRXxDKhXCScKRJ7Bu0DFt0DiqQQ89iRc3y0/CQFofFlZuxcRdgWIpdD532axyTf/R1Pc9Rf593eBXxpsRVZfPXMQcb/3MJtJfhnt4jrjRuNkB3NHiyL3sECW++Jcasj7p9Z4S/UnqNm9pg12syYr1zbQ502Iv2zwX5+aeoxtjeLjF60mb9YRLTa6VR4g/PhVkah2FImF6MxLBFz3NpmznE4ZH0d+YDm+UMzfOX8AUqXCnhbHgQSwgghBUFRcnx2nQfKy6yMyjxRP8Bap4i/rtEr6yTD4Z4wHt++kFaSIKIYOYqxeprn2rPMOkc4Yq/jOy18YWEIgSMsClIzbTXZl2sSxCZbEwX6+/NYXRc3SVLXY5K8uutRGuluy3NJHKi4A2pWn5wIMYRgpGMuRkXOhlOc6U5iDjQyiFI33R74QG4VpOsia1W05zCYLTCqQVhReOUR+3JNZp0ms1aTkjRwhUlbhTSSgKW4xmqviGyZ2B2J01bY7RizG0IQvlTaeyshBDKfR+Z8MIw0QdK2iCs+vVmTwYQgrCiKlQGzpTaRMhhEaZXgS6dI514vdlBaMIpNwthACIOtftqxu+HmqJgD6snaq15GguArnUOsNYtEwUtfbSFhFJl0Q4dESdp9j3BksqoFT/X3Y4mEpajK+f44W8M8vcBGSI22FFHeYDDrYRVsHCnTG8EeDjVqpREkiCDBakvO1sdRWlAvehzQIa6ImPC6RMrgYq1Ad87GKZp4noVlpAu4DsNXartIiXDs1JMgBJgmCIF2LHTOTUMnNZfBhEXiwnACklqEkwuZdDr4QuMKec1INYTAEjG+DLDNhMQ1sHK5ayHEvXBzuNkI10GX8iRFlzinmXFazFhNpFA0koCtuIboGzhthdVJy/1fPi7Sc5HjNbTnMKxJ7PKQyVKXBb/BPqtBQY6whKKvUuOmqyz62sIVMZNGSEkaTJltpgtdwtgkLFmooocsFdDD0Z72aL4ZRKwwQo0RwDCwWY09Ij3kmdF+nu7vx5chSV4Cm7SUTckcsN+vc7I4zbBWxJ2qIaIEs1pBS0FQEVSdPlWzzwpl2oHLcGThhfpaf669wNsa0tL1JnRMas/7XC4e4H+t7qd8d53/+fh/4bjdpCAkeelgYfCwu8TEWJf1cok/zt3LmQcn6NRzVL82RelCFbsdYpxffkXliZHPIaplVCnHcELzwcoZ7nJXmDd6SDxWY83PL32YF87MYW+azJ0NYWM7dYdnnXZfN+LgPpY+MkZ/TqErEQfmlhn3ehzLb/De3FnKxoBxGWBhM1ARfzac4Yudo5zvjtM6Ocb48xqrn5Bb7GFsd9J+Qu3Odfkie+ML8nqQjkP80GGaR12ivKB7MMGZGuDaI6aLm4y5Par2gLv8VWpGj9OjGb6wdZh28FLJdJwYXGkUUXUbkQhkJBAxGKEg3gajpdg0Bb+bO3At7PVqWD1NraWQ19skAhLLITGLCA3VSCNjSGyHz1TezSe9R9PjVJrbEBaBqQQcxfBwwMoJhYokY38+wfj6FsleFshTSerV2m4y9ZUi3eUqTx+u8MUPLXOw/BRTpuIvj32dbtXjmeo+nj46T2fksrxYJn9pHnMA+dUEf2WQJrXukHgW/RnnWq5UlBcoE+KcJqootK2wCgFTlTq+FXKv32G/V6dgjHiHf5GCNDAQ19qoWAhmjTauE7FQbrK0UMFuHcJoDtBLq+mN9nZGCJgap3l/haAkkPv7POJfYMrscjmq8vSwwpfbh8hdMSic2UYMRqjOq5ShH5pn9f1VRjWIjg/4ieNf57C7wbxVZ7/ZI9LwpeF+vt47QCvyeH5rmlYrR6k04P9++An+Qv4Fakafj05/jdaEzy+L99E4V6ZYsLE3eogLV26/6i2tEd0B/noeq2/SWczx6/vfiykS/uj0PbinPBIXPn7ift4xt0TV7vOuwkUeKF0h2S/5nfc/QudABS1BG6mwrHmizWOVF5ky23ytvZ+NjTKiZWH31J5yJLx9Bk8cX1sorbMmM/E0UdFiSda4eHCCCaOHNCJ8FJYwOGoZHLUGRLrL4/5F2nMGf9y7h/+j/zhC2fhbBoVVD15eauu5JJUCUcUlrsQ85F3e6Vqc3iUayuXU5RlqT5q4TYV7aTt1k2a8IcKJPMEjPT567BuMWV2OOOsU5IhxOWDGFDsdqdOwSqQ1z/QX+OLqQVrNHNXzUHm+gxiGsLFF/O3q/OwRhG3T2efSeEBhVkf8zXu/yN8un76hR5i8LoOgIId83Vy4weBRGlTdpnjeQIYvLRBWX1N9tok+df6ars6rae5c5QbtnddBYed8wvOQY6nHrnu8wnpBErsJs9NN/uq+rwLwC+t/kXFn9zsef0u0Jmm2yH/DJHfexwjGePE9kwxKgrKM+YBXx8Lgh3JrqIknGOiE/3X2UT4+dj+9rkOUt9HSRyYvjWVQMOgcFARjCdpVeJUhnhMyXejynurFtEO6vcG9dgdfGNcqwyD15rx8qbWEpGpEOKLLvlyDs+OH6M17+KbE2rDgNrd3AOKyR3dfqrlzZGL72qb3mZHPk50DvLA9RW5NoS8upmHuV/H8BhM5WvdHjM+2+MD0eT5W+RqThrMz/h5NNeLsaJovrh6k2/OQFzwqyzCYdvnq+AHe45+jICP+gr+ELy1Oz87w2bmHEdqhGGvMK+btZ/CQNr21t/oYIwd3K8+TG/tQGnLf8Jj9bIs4b7MSFvhKvJ/JaocPFM/wbgeS0jOcPjHFxYkanh1R9oZ4ZsTD5UUechexUISJgWhaOA2J2YvSxW2PsDtl6WGE0U8nkVu3+cPN+7hQnOCQu8ld7go5ETJphEwaHoYQuEKgZMKM1cIcH9Hd5xN7Js72BLZjp96AnYah0WyVzgGPsCDwxzrkRLrV3VYhLWVycnQI2hZuKw2lcBskx95UpIGRz4HnIiwLXcqjXJP2QYfJcpM5u0HZGFCWA3IiwhYKpSUjYi5FkgvROOtxiS9uHqK5WsJspS5qMQzTaovbIW/KMIjyYI0NGS/1mLFaOMKkpwOeDT2WohqRNmglPpE2eKq9wPMr0yQ966VzKIG3YeA2FPK6KWmOFGIQoK4LI73VHTa0AsIQRgECsHoJdssAYdEdc/BlgCsjtAHcKs12lUZHEWIYYA00S70KJ4NppswWvjXElenSZ2DgoJl1mkxXOrQcj3avjFAG17eOi30YTcdY5RGOEzNR6FGwAub8FjN2k6rRoyyHOEJeM3QTrYlIOB0aXIjGUVru9JTTuDJk1mhTkhETdpfRZEJnZKANl/J6FUNpdBimSaF7aIf8ZhGWnVZS2Ra9cZvRuELVIqa9DhYQobkQTHKyPk2zXmB2oNGJusHrK3M55OQ4ynfp7rPxax32FZuMWT26ykAScCX2uRyNsxUX+Nz6EZorJYy+xNsSuE1FlBOsDYosxVWqRo+q7CORmDIBkRYL7HofhJtJHEMQIg2BW9dsL5dBw3hDIwYBJuA2PMI1l/VEcGZmmhV3lZGqMOl2CUomOSug5gxwZIwhFKeCaUJtcKExhrslsVtg9t94S6ibya4YPKrXRy5tYNoW0+Ysq+39XMkfoL+QUNvfpOoN+NGZr/Njhcs7OT02vta8x73Cf//gn3D2+BRPNha4tG8Wb3MGGYEx0ggF7aOaQw8ssy/f5IPlF5kz02TOT/UP87nmcU7XJ6icEhSf2UjFBput3RiCWwbpuahjC/TnfEYVSfMuDdMBU7UN/vrCF3inu4gtFL5Id7KJhq5WdBODf7X6PTxx8ghmxyB/WbBwOcIcBtjLLdjYTqUGboPdk3Bsugvw393zeRbsLe61NzFEnm+Mcvytb/wVkjMFjIHA29aYQzBHmvlOggxfWgiE1pjdAbJ9Y86AiGJUo3XT34OO4vS7YBh4QjDJGFHeYDlfoHZPj3mzReKqNH/lFkDHUap23OuTW6tx/twkv5g8zkNjS/zNsc9TuO5mZmHwmH+G+f11Bsrh4oEJloMKyXV1zJ4RMes0qRp9LBFTNgbYIqEgh4wbQ1yh0+R8UiM20gkjnbCRSP7p4l/k+ecXIAEM0FLj1Ib8vXs+y0eL5/nuwkm673O59GCNJ188gIwnyC0XMLa7iCsrb60S9C4jq2WSA1NEeYutB0ze/egp7i8ucb+7iLtTmfUHV+4h/FKNakOTu9xJPTvX5WqKhVkWv3eM/nxCbr7Nzx79Eve6S/S1zclgho7y+LXLj7J9cgKzJygsag4uhsgwxOyMkL0RdnuMiycm+XjuIY7mNpkqPUXlVjHm3wJUf4hIthGmyUSsKJ/PA2CvNmB9C2FbTHxNULro0d3n8luFh7AOx/gy5HsqJ8nVAiwR44p0d/bp7j3860vfSbPnI54qMvvnA4z2CLHVINlDOZm7YvDoKCRppl9iK0mYbI6jfIv6/XnqqkK9lOfFyjRR/iKOABMDR0oOSJsD1iZJYZ3PFE7zPwY/wEatBIGB0ZegBDP3bPCvD/82R63czqt5bCd9zo6meG5zmu5Gnn0rCcnFxdtfa+EtQFgmw3GX7rzBcEJz+KElfmT6KaasFg/Z24wZ3rVjr5b8dxW0lMPJzWkqzxi4TU3xQhd5YeXarnUvJ76+YQyDuBrzY8VTTBg5IF08FqMqyZkC01+MU2HLs0tpues3QcNrqsfeVK52CQfYknha4/ou9sNVciJkXMZoU78k1b/X0fpaexSzNcLd8Fhxq3hWRLdm3XCoJQwOWwaHrS7Qhfz26xYpTEMnN7avUCgSNJHW1JXH6ZUpqt+QyAiUleY99PblWTxSwxeL3GVFTNW+SL8i+YfJX2Lx2YMYgYsXJAjLvK0MHuG5jMZcgrJkNBvxkxNf4iGnhYFACkFXmzS3CsyfibFbMcZ2m/hl63Rc8emdCHnXsYs8WFzihwovMGfmeSZo89nRDItBlfVLNWa/pnCaMe6lOvHlpXSOAwrwXBurXuNCewxDaLoF61Wv93ZFR+FL86rZxDiV/vfaSAuBHI7wPBezP8/5hwucnJzlaG6TD+Ve5JB5/bqv+XQXVlerGA2TyQsK47kLr6/1x9vMristE0bI/hChFN62T27RICoYfKZylFmnybjZ5SFniYOWlYoG7uRClI0BRyubCKEJIpPByEYpwZHSFr7QJFpRV0NWY5OVpMaXNg7Sv1jCq0vs1mBPudn2HNLAnBxHV0vEZY/W4bTEVldDDhe2mbJa1GQfa+fm11MBq4mgr02eHh7ly+1DbA4LDC8UKW2loUPZHaVGzlUBydsIHcc46yb/n833s9+t84HcGe6zXSbMLtFsSOMuB3/doLaag1sgX0yOVek8MEVQkoQTMfUkz0URIwN5S4ZXRH9EbkUjlM35ZIpfyX+Qu/MrHHfWeJfTJC/TvKSra4tCv0KdOdIJXRUzenkLoevsv63EZiUuM9IWy2GNlaDM0rACix7+dgIaRmWDyBUoQ2PJZKd3kcQCXKGY81u8OK0R2kRGPt6F3V+i3zKEQBV9OvtMghrkxzoU5QgDQUspGsrmxWAa2TXTDUI7gCi6Jmwqx2vonEdzn0u+0mHBb1AyBmwnFpHu8dn+ffzO4kM0uz7+konTCLA6AQThtfVemKkgpPJt4pxiOtdh2mnjixgw6ccOVhe8RoLRuTOq5b4pSbKTfhLhrvt8tbKfzfEC35k/xaGXTctIG5AIRCwQau/eW3f926QGA0Qcg2FQbPcovJhD5V02Nmr8b3d9GFkJ+b/d/XV+tvoErhD4wsIRJkfMiL8z9Rla4z4KyUhbJFqy39qmKm1iEp4MavxJ834u9mrUn5jiwOdHmN0AubxFcidP5G+B9Fz6D86zfZ9FWNKU7t3mo3OnmbQ6vNO7yJw5vPZZAFyMTX6/8yCrozKfefEYlS85uE3FwdUR1uJ2Wnk1GKL6O5mYt5lnTff6TH855pOddzMa15z7rgl+cfqr3GXX+esPfZFnDs/x5OmDFC9WEVf2fkvswbEJVn845NjsBvd6Hc6Mpnkq2Y/dlqlS8y2GXttk8rMa5bsMDhT50uX7+bPSfUzet8EvH/+PHJU3JpVLBOq6rj+J1nRVzJmoyHpcesX55U7e2hc7R/ny2n4GI5twLYe/KrH6MP9igPfiOtp3iR8cZziZCnP6MsQQEgeTkoQCiu8onuH5h6fZOFQgsfPMP+vDXq6Ke73syIUM5wt03jPk2OwG76tdYM4cYgmTc1GJP+8d58XuJLlFiXNuPV0z4jg1dibGaL5rht6cpLeQ8CP7T/FDpafoKJcXwhm6icuvnH4vuT/NM1FX+CtdjCsbEEYkV0vLpYHM5xC5HIOaiz054CNjzzNltShLhUKzNixSuhKTf34D3RuQvFye4E5Ba1QYIeIYY22bya/59JZynD+e45mpBd7tXLl2qEIRKBMRSowQZPxSTu1eY9cNnutVM1W3CyupQm25djeJZzHqulzcP8agIgCNK9KJmZcOD9oAL4kQpotW+pYCHbMelTnfHWO5VSa3orGfv5J+ibLy89dEWCbDmslgPkFWAh6bPctHy1/DFwlVw8AVzg1VKF3lcmlQY7FbxVpyGH+qg7HeRHd7xN3ubb9L0mGEt9hmTBfpzlmcfecEalpTlSYfLpzkfv8KS90yiV/e/S/c6yAoG7zzwAV+YvLLLIU1LowmWBuVkAG3pGdU9fuoi2nX7fxgP7E7yagiWZsu0T9m3mDsvJxEp1L7EVBP8qxGlVccc9XgOd8do7leRPYNCpcllXMRVj/GvlInXlnFqFaQ8RjKAEyNFOlYXjV6FJr91jYPjS2x7Jc5WTmS6v3c6giRVhZKQZg32D+1wUcmXuCYs0pBphVtjSTP5UGNlV4Ju6NR9QYqCHb68tlo32UwIenPKeypAXd7yxy0RlyM4FQwy1pYJtj0mT81wFptottdkpf1jhNSgGWDY5N4kmJuxBFnnbIc4e60YelHNnY7Rm1up/elW3C+v2WoBK1A9Qd4y13MgU9QcWnGuVceqgUkpB6ePbyf3ZPfJp0kOFsDilfy2G3JE+Uj/Ez/x5nJtfnh8ad4l7uKLQR5Yd1Q+ns9CsXZ0RTn1yZQTZtSO+0PpG/DkMpbgjQwjhxgcLBCWDRo3AsTB+pM5HocddcpyASLl9R922rEamLQUi4fbz7MF144ilm3KF8E2R6gRzshrNvc2AFAK0R3gLNpoQ3BmRdn+WvedzLpdHl3/gL7rW2OlLd49r5JauJhnO0hXFjaUzFuY3yc6PgcQdWicZfgEbdDoiVfbh/ii2eOQNti8rJCB7foZmGn11VSzdNZMAjGNBM7VZwKmRo1OmGgE74wnOap/n4CZdKJPYaJRSv0uLA1xqj3zUWQjG2b4qrAGGly6wnu1ggxjNDDnU1ZFOM0Y/yNVCf7dy4/TC9x2WfXecw/z5zpUZIBx7018kbA09VDxNMVTKXQ3R7JLbp5kJ6XhqN8l+G45Fi+yUFng7IxYKASIhK+3D3MExcOoJs2s/VUu0WYFsb0JKqcpz+fp3tQUT7c4FClzqzVRALPBfP82rlH6TR9iucNzNYQ3R++qqaa8DySQ9P0Zz06+w2OFpvU5JC+Nvn8aIxO4rK0VeHQMA29c4vpgd0shGkSFV1GNYs4D7585diaUqEtjbI0yty7eX570+AJQ+TZRYrLHiXPpXx+jNHYJM/vm6H+vTlyC5+iavQ4aIaUvonBE2nF0415nOc93Lomv9RH9Ydpy4JsEr8CaVvUHxmn/j0jKqU2Pzhzju8tPUtBjpg0QsrSRCJ3NEVgKxH8Sfderoxq/Mnzd7Pvv0n8S01kt4/a2Lo1FZO/TXQck2xuIVptcpsF9sk5Tj5/D1/Zp5Ef1nx4fJO/NPY0Lzw2xaW7ShROl5lv9faUwaP2T3HpBx0Kh1scLze5y18l0iZfvniIhd8UeMtNRL1F3L8FBWKESD0FhsFgymdw/5DDM1u8f/w8JZmgUAxURFcrlmKfX7z4OBvPTGIEAqtHWlnX18wshtjb3/z9i7CDGAaQqLT57WgESqHCdM1RQYB7uY7VLZBfcagHY/ynye9A7BtQeHjInNlgyoDHcmfpehZ/NHc3ncNlcjkLe7mFGAxuyWR/WSwwPDLBqGbR3a94X+kc73TqjLSmoQy2khyfvnyMyuddvIaicLZFEsUY+RyjQ+O0D9j0ZwUPv/MMf3fmU+RExKQRYQmDT9dPYP5xmYPnQ+zNBnppDT0c3iheumPsymKBzftzNO5PcMa6PF47zZwJXx35/PvV93G5WcE852O0t76p7s+diHBsRhMO3TmDYCyhZNz4HZDI1AiyFYlroExA7s2a/j1p8KB1ejPodhGWjZMorO08WpTY7OXpKwdXRCi++W4zQTOILOwO2F2NHEaol3V4zbgOwyAoC+6aW+NYYYPvKp7iHc5gx4N2NanzpUkcaIPtKM/6sIBsW3jLXTh/mSRRd6RRebUiiCTBXyohoxzasGmEORxhMWW0OVHb5IKZUN+aQDuvIZf8drJzM4jzFmJ6xPtnL1Cz+rgiTPPiOhb+uU3ii5d3+0q/fa52MTcMYk9QKg04UV5nn72NvWPAj7SiqwzqSZ7NVp7cqsAcapyWwuoprG6EfX6NeH3j27+OJEH3B0gpcLQmt24iY0nHd2nEeSK9hSUk41JRECFlf0g3VyHKm1iOnfYIuRUxTaKCSVgUJPmEcbNDRbpsJ0NG2qCjXIZdh/H1BKceIDr9NJRkpAUsQSVtzXJfcYVHHL2Ts2kQacXWME9hJcY9u47uD0j6g1fkCF797LEtgrLAmxgwXe4wZbVxhclIW6x1i3TrOQodEEGUGTvXYxjEriD2QTsKS9xodCsUhlAIU6FNjTYyD8+3jbBMVCXPaDrPcEwyke8xazYpyBDnNRYAS0juqa7xyXvHsBoGTiuPe8pA32YJs28W4ThpIl8hT1SAOb/FPqdBWQ6uGTjXGzpXKcmIB3KLjFk9zu6boP5gkcL4PVitAHOzDVF8S7vhv110FGM0urhaU8iV+Oy5o/y/zSFSaPZ5DfZ5DX5zskJczWGWS+hR8PaKywmB9Ly0cWOhQHAo3Xl39huUi9sAfHn7IP/nyrvRfZPa0wa62397ru0mIXM+YmYSlXfozRicqG3xUO4y++30/bZVyG937+G/rd7PVjeP9Vye8vkII1CY/Qg5jBCjCD0Yvqnr0ErDTnjLSBT5RROnbZE4Fn/WPMZ+e5uq0WO/mWAJgWdGNHxBlJNo17x1RB9fjiFJbEHiCDDTmyPAljJ5ZrSPS8E4RsPC3R5g1HvonbYawvdoHTQJHuozW+1wj7cMwPko5nfa7+Bcb4IrZ6c4Uh+i+4N0w3G1GstxUoFDxyFZmGQ44zGoGQxOBHzX/CXG7S6Jljwbwmc7d9F6oUblMhSW45fm+x20br0CIZC+j3AdkukqrcOS6PiAmWqHrvJ4MtAkiJ1iIYdESw7MbrNdytG/XKFSKoJS6DDaU7IKe9/gMU2C8RzdeYvBNBwpbnHQGmEhcMU3105whclfrH4D62HFmc4E9cU5PEOiM2HlG5D5HEzUiEseQVVxd26F484qk8YQ42XJydczadg85l1h5F6hfcDjNx95N52DFv6qTeWsjdmPsdbMW9YN/+2io5BkfROxtU0hmKMwN8F/qT+CM9PnH937J3zIv8wT+w4wmpiksFGBbj9V+36bujILw0DkcohCjnC+wsoHXcLDQ/xcwIFyHYCzV6bY/9vgXdiCbh/18vYttxgyn6N/sMKoZtA7oPhQ9RTfk0urTBKtaSnJf7r8ToJPjpNrakrn+phnl1KjPUkgSVBao9+sKrhKSLpdxGAALQOr2cIyTdAH+fqD+yiYAXfnVxjPP0/VMCjaIy4XIBgJEs/CFIJb8hYsJbEriHIgnQSLNIy4Gpf4YvsIi70K3rrEvLJJ0mim46w1OufRPRHxS+/4XWpGjyPWEHB5JpjjN555F9aiw9gFMFcaxO1Oauxo/dLNulJCFX02HynQui/CKff4iWNP85Plr9LXJqeCab48OMLnlo4w/eWEwlevoEcjVK9/Zxs7gDAtZLmELuXp78vBvV3+7l1/ToKgHft8unc3g8SmlzjE2mDc7vIjM0/RVS6/fP5DqGoBmSSoXj8zeF4XQqSuSMchyhtEBUHsK8rWAH8nb2ekd6q7tCbZWQosIfFFGi6oyT7H/HWGicWmO5+6NYW44yfzDUgDbRkoU6IlKC1JtCRCpKX74qUqhZf3BypIE39Hlt+ujAi1i9lP3dcA5l4J27zNpKJeYPaHOE2Nuy0Z5hy6KhXrkkITGwJtGghDIqS4+cUgO2XBqWcnR1LKEZQtgopiotrFNWOUFjTCHKJr4i03Sc5fuskX9TZhmmlopCRRuYhxs0NJpk1t+1rTVg6trkdtU+E2YsytDkmzfXPkE7RONwBxTBLFCCmwujFJ32JtWGTS6aBIiwM8I0LZmsQRKOsWDWdB2lVegpY3RuVG2qIbOfRDGxmS5j1dVV4XAkwDIxdxr71JTopr634r8dP2QNsCp5OkCcoqSZ8jjbQ3nO+hij5xySGoQH68z0QhLcCYNmw2kpBW4nNxOE6v5zJRD99cuPJ2Q4q0mi3vEOYk1UKfu9xltuIiz4Xz1KMcw8SiFzkoLckZAVN+m7IaoDyNci2k6yD2WFn/njR4hGliTE+RTJQYjvusv9ugcM82dxU7PJo/hyUMluOAPxscZjmsshqUON8ZJ4hNvmPyPD9b+zJj0qYkAw46G3QTlycKIMdriK6TWp23QUuDtwSVIGKFEST4a5L/37n3MFHo8YHxc3xn/hSuiPFFjCsUhgALsITY6fwssTB4wL3CRw6fYnVY4pnyHHUzh9U1GNMl7CX7jvLwXI/u9Sif6+M2XYaLFr+0+X38fPkj5K6YTG+9ufDIG0FYNvLgPoLZElHRoH7CZDiXoJ0Ev5K671frJVavTGO1BZOXFaLeetuu72aTjJXYekDAgT4Pzq6yz2wCklORyye797I4rKIv5yhcGWA2B9DqvD3lyFqhlcTsjPAuV3hBzdHZ7/J9xWeYNhKO+Jt84dCITsXG27awLQtGo2993luEVuKz3i/S7PrkRzod86veGc8lLntYVnJN3HGwU0W3GNRwNwyKSwnuZrqOS9dFFApQyoNj07ivQvOEIMpryke2eXzuLFUzbQvyjdDkq4Pj/O/PfADrokt5Baz19d1TOd9LXE3wLuTp3jtB67DJYFrxF8aW2G+2uRyO84mlEzS3ChBJ5EiCgosHq0wfbuPLAFEMaZ7I40575M/a0O7sGe21PWvwRHM12od9BpOCw+++zP9+8HfIybQU3cRkJcnzXzYe5FK9ymDbJ3fJwhjC776ryPe8+zlK9pCyVBy3tok8k7CkSGoFpGkgk4QkM3hSlIY4QYQxhSVF8xsVLuXLdO928PeF5I0RE2aHshxgiZiCDHFFgi8U5Z1GiffbIQfH/4yRhl8vPMJvGg/Tabm4DZva7aAj8m2StDuIb5zBN01yhsGY4yBMA0wT7dpvW5sGYVsMDlXYvtciqGne+d7T/OPZP2YpLvOftt7F2eYE8bbLwT8NsE9eQQfhrVmN9U0Ixz2qD2zx/zz8KabMNnNmTKItnhkt8PuX76Xd9qleAPPcKqrVfvuS7rUGnSBbPcoXStgdiyWjxubBAtDiHm+Jdx+4xPJ4mcalGSr27dX+oJX4NPseYdfGGJGuRUIi8jkoFQhLNq7d5eq77ipNqCVLgwq5NU3+bBsRhKA0wvOgWiKYKxEVDDYfgY+872kOelvc713hXrvDSGtOhmM8PTzAn2zcTfWzLhOfW4VRQNK4tcO2bxVXE7xFsUDjuIl6Z4eFcofHSy+wz/ToKpfO2QrVcwIZgt3XCKXZiMtcnB1n3m1QLA5pHXOxOyZ2p4B57m3wYL9O9tbd6Krb3fOIimlGfVjWzOeazJkeEkFPB/RUxGo0x2qnyKDpYTVN3LrGHGo6HZvL4Rg1uYQrwBKkFV22JvEsROQgrDsz1PKqaIWIE4gSzKHCbklELNhu5XmmMkfRDKjZPfLGCFfEVM0eORlQlgMOWh1KUmMgqBoOidaMWV0cOyZ0EtQdbOwAN/Rzuh6ZyyHHqmjLRN/EG6vM5RD5HCLnM6yZBBVNXIop20MkmlAbNIIcrZ6H2ZNYzRHJdv2mXc9uoaWg4ATMmk2qcoS1E1cJlEUQmejAQIZAEOxOvkGiMEKNEWiIJeq2btP9EkpL4tiAWF4fOU95lc1AgiBCohBpN3NDoB0bMVYGpYkmCgzHLcK8QJVDjvnrLNhb1GRqvEcaVqIqZwZTrHcKlNoKtd1Iq+febH7WbYQQabg98WCy2GPS6xJpk7VkyEpQwepJ7E46Z61egtBgDgwaoU/RHCKERjnXhWL3UHXh3rkjCYExXoOxCnHJZfMhm+TBLuPFPu8tnkMi2EwG/LfeMU4NZnhi/QDhl2tMLivsboK3MUJECUG5xL+a+DCzpTaPT57mx4rPMm50MSeGNE/kcNoupSCGra3dfsd7AjUcIZttME3yWuM08iSOZHDF5dmxe9LYuwnKSBsfRkWF8jTW2JAfPPocD+cuMWs2OWYNr7WayHhtdBih2x0wjDRv4SaUwArLZvgdd7H+qElU0Lj7Ojw8tUaYGHxp5QCfOP130D2T/EWT4pbG346RG43X2TIz4+3g9GiWr11ZIGo5TG4quM1uyt3EJRxYGH2JGejU+NcKhiOEEFi9AmFspKEmrWFH9HTC6fLsvMAMSgQlyWBak7iapJSQq3XxnZDvn7zMve4Sloh5IZzhU/0yS6Mqnzh3F/qKj7styF/poEc7lV17xQWxm1zVq/JcVNFjNB3xw7PfoJu4/ObGI/xiv8jKYo2ZU4rCuS4iSRDDdIOQ2zfJU8vzXCpW6Q0ckpwCJUjsvWPswJ4yeCSUiwz2FRlVDUZ3Dfl/3fNZZqwm99qbgM9GYvGJrXs4tToFV3wWvjjAenEJwgi1U+5ZK9zDaqnEhWoB5x0xHy0+S1WGzI+1WDqYw2lIcks5RJa8DKT6MUkQpDuqegPjnMAQElsKKsZ1oo5CIPI51PwEYdmhdSTHJ5wT9Gcd7sktM2+ewb9Fq2bfbnQUkrTCm5pAL2yL7fss/tL3f5H7/CWO2+scsBRPjMr8rZM/zcxnwW7HeOfXUeub6EQRx1kJ415Bobg4HENf8cnXBd5WeFvlwiVa00scGBoYA4ERJmn/Ja1Rw1Haw6kXEkXGK4zwmtVnNJO2nh/tC/nu+17gRG6Ng/YmDzib+EKgeKkC79OjSf586zBrrSLukznGnw0wuyHyygbJHqog2gsI00TYNknOojjZ4ydLL/L5YY1///yj2Kd9auua0slt9KUltNaoKEYYBoVDNVrLPuslGyQILybRpFIEe0hOYW8YPDsVWSrvMKoaBBWJlwuZstqUjQEjLdlMBlyOp1hsl4m3XfymwOylTSl1kqRxd8AYRNhdF21KOoGbVjwIMIW6VinwGq1z7lx28gmu3+i8/FYstUb2S5iWgQxttNAUzSGujO4QJ/xbzFtl7FytaBQSWcwjSkVU0SeoaCbsDgVjyHpSZD2BJ/qHsVoSpxVidUJ0f4i6jRJhXwvjZTN6oGyCoYUYSoxQ39Tw4quyU1GkPYegKAmqApmPcEW6loXKREZgBCCjvduQ8dvBEIKSOUTkY+JQEhQNCtUKwnMRnod2bUYTHp7bJ23EARaaBEXV7GOXAoKRxK8MWXAb7Le3d+4Vgkhr1hOflbhCI87z1cZ+rmxWSdo2Yy2N1QmRgxAyY+dGhATPRRdyRL6JY8VYGCgkSd/CbqcivmIUpkrUSqf9tnRa9GIMTJRloHMJlhsTKoGy9pbXf/cNHmkgLBPpubQPFdh8l0ZUhnxk/jzHrE0UgqeDeS4F43xx+xDBV2rMvRDjtIbI9TpJGN4gI25ud6mcdQnKBivHyoy0wBWZJ+etQCcJcjDClBKhfA5U6nxv6RnKMqAgX73FR8bNR9g2slhE2Bbtd8+z9l6BKsecOHCFcbPLVlzkl848Rv+FCnZHMPNMhHt+E4JwT7W3uJnI64ydq/3gzvYnsM97uFuQWxtB9DZ6uKSBkc+B4zA6UGXr3QnzB7Z4oLbMvNkiAdqhi90V2G2NMYzffoPsJiKRvC93lvY9HivDMl8xjiKjOYxI0580GNUgrCq+d+48BWlgIHC1IkHzfv8s0d0GK4fL7HfrvMs/T00GnAyn+KP2AzTCHE+s7Gd0qYAxFPhrgqn1NEfRW2khNxro6KWoQEaKsEzUzDj9A3l60wazuR4KxXpUwlu0GP/GELMboFudV4ThrcaQ4iWHsGTQOaF4cG6ZVuCxWdlHyXj5VmP32HWDR0iRutFcl/6UZO7YGodK23xn6TQLpklDhVwKxvnC1mHOLU8wfzIm9/kX0UlCPBy9otxNN9t4l0zsis9a2yXSEncvt2+9lVAaPQoQUiI0HMjVecTRQJoEHr1N4nkZNyJsG5Hz0L5L86jBX/rgE7y3cA65I/l+ZjRD/1SFg/+5g+wMYbtJ/LJO0ncKxnWbn7VBidyKJr8aY2/1Ucnb50ERhoHwPfBchmMmR48s898tfJqa7DNuKJQWDGIbcwB2XyFHt0846yr32F1mal9kK/FYPlpma2saGcPgQMTMfJ1xr88HimdwRdrHz9nxzJdsOGGfAVLDSWKg8PjS0OWJzQNsd3PwbJH5r4VYnQhrtUGysoZOEhSpblvGKxGGQVh16c4ajMY1426PBM12XMDb0FinF9P0kcHgxrVDa2R3SH41T9A36ByHd5cv0o59fi+/8LZVo74edt3gkfkcolpBFTzCEszk28y6LXIyYKAjGsrk+e4MFzbGkFs2Zj96qTHlqyWaaQVJqi2zZ8zKWxkhkI4DloXwPfRUjajgMKoK8mZafZR2mY6ItGYtLNPvu9C1MEe7ECa4QxCmmapkWzZMVOkdKBHlJUFN0YldzgWTrAQV1kdFlrtl3C2B7AWIYYC6U7rYfwtK9pD1qkBGJnbTRd7sXIOrbT0cB5HziebHiEo2vRnJET+Vfhhpi2cCl4F2WG6VybU1VjdBjkKUukU/syjG6SoSVzLoWnSVR6BTz6IrNAUZMpdvsTQzjkgE+bE+84UWE06XstG/Jnga6VShuaFiToU16kmekbLoKpdIG3xu6xhLyzVEz6S8rbE6EUY3eKkwIJvzr41Ik4wTF5QN9SDHN4IcZ/sTmCMgTHuM6Vebh0mCDBVGKBGxTCsglZneg/fQuO+uwSMEzEzSuL9KUBJEJwb8yPjXmbfqGGhWE4OvDA/y1ecOM/FlA7eV4F7aJr6q3vgqA6kThQwixChGxIIkS9h5UwjTQk5NkNQKhGWXxnGH4QSE+0eccFeBtBfRxdinpXye2D6Adc7DaYC/Hry9YYI7CFkuER2fJyzbNE6YmO9rcKi6TSF0+cbWHE9E+xm+WKb2vMbqa2YvNmFtM429h3de7oLaWQcSLUjQSOA9lYs8/45pek0XoX1qLzg3VZ9LOg5iYZZwssBozGLzIUk8FzA53uD7as9y0Bzwx/3D/Psr72Grlcd6Nk/1ZAu53Ub3etfyFG81VKtN8QWPfM4hLBZ5fjjHvc4avtAUhKRgKH5m6vO8o3SFSBvM2Q2mzBY5EbJgDgGPSCe0VchIwx/27uaXnnkMVl2snsDdBiPQ2F3NgWaMEQRY2wPYbqY36eFwT9109yxSEhZT706SU5zbHOefht/P5eUx9m/GqOEoNRxfRURQjwLsxhCZOJhdl6VRlWFiIffYlN11D09c8ujNSoKqZv9knXe6q0waDlfiiNW4wGJQw180qT1dRwxGqW7Ca6k2JglEESKKESozdt4swpCook9Y8xjWTHr7NHp+yIGJBlNmG4CRhvW4zFZcYKNTwN0Er66w2qPMw3OTEK7LcNJhOCbpHY74P+7+L3zIG/JLzcP825PvJ2o6TD6vqf7+KZJOJys3h1dsfg4767xjbonlcpnmizNp65mbiWEQV3wGkzb9aYlzosUPLJxmv7vNw84KVcOhkeRYXaxhb5iULinE0jpxvXFzr+smo/oD5OIq0jLx7ruLrbBAV1lYMsSRJpYweK8b8V733Ks8O23HolCMNLSVxenBNPYpj8rZBG8rwjm3jup0IYpQYQRapW1xMt4Qwkh7nsXFBEzFqONwuTuOtW5htQevrVEVRohBgGFKjJFHK/IIlYHcY1kOu2Pw7MhXC8skLNmMxjVxLWIu18Ii7ZH1leECn2ud4IX6FG5DIwYjGL2Gx+DqOWcm6R8dIygZiIk0Ka2rDDZ7+bT3SkNj9MPsBvBaXCfvLgp5+nMFerMmQUWgZobsG29yuLhFQY4Ag6Ukzydbd7PUrzBYzTNVV7j1CNkLUDdBYyYDiGOsXkJiC9w1i1+88iE+UV7nc0tHEBd9ch2Btx3dVqXM3y4yUjQGHi8GM8xbdcaNFq40qRk9DuW2cGTM52enCB46iNUJEWGMiBJIVNoLKIzQcYIejV6Xd0wIgSgVoZgH00DlHBLPIsyZtA5aDCchrCbcU22wz2lQkEO2Eo+RDjnVm8HeNPHXBU4zun20d5K026HXSPjk+eMsD8rsz9d5ILeIL4NrFXSWiJky29RkgCWgJA18YSORWAJ8GTPjtBjOxghlEpQlRXMaqzeGMiXKNhBaY2/2YX0r1bwa7ZKg5C2CMM304XlEBYFVHoEWJBsedlOmHrRu8Nr3zCRBhBFyaGJ14ExjgjiRmP29VV24KwaPMIw0ju06dGdNJu/f4L7aKh8snaYgTRoq5t9d/g7qT0zhNGHiZB+1uZ3m7rzaAiAEwrQQlknngQlWvjemUuvwA7PpjmEpLtNeLnHg6SAtSdxoZIlrr4EwDMT0BNFUibBis/mQSXBoRKE45K8eeI7vyL9IQY6YNyMiDX/eu5tPfe0+3HWDyYuK8tOb0OldkwzIeOvRgyHeche77eDVbdrn5/iSPU+pnjC93EEOI0Sj/VL49w7GGMY0Nor8Uele7i+tcMj6GgUJR6wh5dLXaRUcuu92eLJwEDnMYbUlVjctB/e2FU4rwRwm2GsdRKvzLV9PmCaj49O0DtvEvqA/o9FTI2x3xN1Tl3igtEzF7HO3s8Kk0aOhXJ4L5mnGOb50+QCTTyvyFzrIRpfkdqgkUgkqUCAkhWc3EMkk66UDnJs/xO8fux/TjZBSIwRYVswDkyu8p3SBqtnjAWeVA+bVZsUGtlZ8R/5F1t9Z4vJdVS43qrQuFTCGNlFF4UykqsrmkzVm/szGaA8R9SZJvZGFtb4J0vcRxQJqrER/VvP+/RdZ7pdZfWofs5/vIvsBrL52Y1UVBMhWGzEKKF4ps3myBhpm1mP021gM8K3YJQ9P6t0RlkWUFzxcXeO9xXMcsrawhMFIJ6w3ioydV7itBGu9Tdzvv/b5DIkwTYZVyf0Hl3mkcpnD7joAfeVgdCXOWhfRHaBuoz5BNwXDQPsOYcVmWDUYTcYcnNlmX77Jd+ZP86ib3kQTbaFQrAZlvFWDwqIivxSgVzdQr/V5ZbxpdBwjewOMOMFoDnCvAFojOj2S7TpJ5tm5hogVom+y2isx5XUZ6TR0VZI2JQkjHfCB6lnqh3J0Ri71ep6oaSNHAi0l2gBrIDG7DsbI+Zavpy2TUdVkMC2I8grvUIf3zF5i3O7xeOEFHnT610rjQRLFIc04x8XhGFHLxV8dIi6toMLwls3beQU7Ol9qY4t8FJNzbaz+BFuuQ5yzSCQgYeQoLrhjzLktRtrikJUq4kvSRsUImDV6PF5+gc18kaf8Bb6gDjEa2ExPtHh8Oq3e+o319xGVHUSiMDp7SwtmTyEEWCbaS72QcV5xPL9GoEy2uxrx4uV0Hn4LT6OOYrRI7wt2O8bddkCD1Yv3lIr1rhg8Mp9Dz04SFxyCmmbBbTBrNom0wekw5mw0g2o4eNsxdjuEILwmriY8L3W/OTa6mEfbFknRoT/hEHmS1l2aj5RWOeyuY4uE1bjElXAMcyQQwyBNYLtdE2mvVoEszBKN5dGGQJkCbQjsZoBxbpmk+c2b5BnFIqJYQOc8mveUaR2VRAXF+EKTh6pLTNttqsYAMGgkAaeiEvUkz9c35/HXNbm1CKsxyLw6bwM6itGDUdoH7frfB+GrV1HcwRitAaVzBRqDcT65r0DV6nOfv8is2eSINUQC81adh6pLdGKXK36Vei3HKDLpTPh0uyZGILGPlTAHpW/5etqAwazCmOtRcEOOjW2y4DbwjYB6kucbAbSUzzcGC6yOyiz2KpxbmYCuRfGsgdEc7FSiqtvOK6GTNDQokrQdUPGiT+KmvbG0THsvbXYm+a3JCpYX8ZW5gzxSuowjI3IywBURI23RSnwCZdGOPOLIQMeSjXqJ/zx4gCSR5BYN7HoP2R7ckUn6bwRhWSjHRrkG2tIU5AjPiNJ+ZUmSPr6F0WKM11Az4yS+RW/WJiyCjCFxJMYd30urVqZzV5lRRRDuD3hX7jx32V2eCcp8frifF3rT5BYN/BdX0f1BKjW+EwaTY1VUziMqu3T3u4QFQX8GjBNdJoo9/vL4ZT5a/hpVmXAqKvHscIEXejPYbdDNFqrXvz1vyDuNV2WlzOa7x2jeo1GWRrsKYSuciwX2Dybhmxk80oDJMYYLFYKKyfr7FR95xzNM2h0e9C9zxNrGFYrqTmLnUuLwn7Ye5UJnjMapMQ6f7CHPL6ODAJV1or/p6DhCNZuvaMz3zaoo7mT0yjrTn1Bo36HxQIX/lLyTz00c4Z3ji/x09UuMGRH32pscr22RaMFg3GSkTSJt0tc2feWgtKSjPCL9+hKba0aPcbNzg7pzX9ucGc3w5eAwpztTvPjcPnLLEqelOXR2hFlvI3sD1OY2aji8WcOxq+gwRLXaICRmf8D4lTzI6+awaZDUCkQlh7Do8dw9x/nqwkGkk1AsDim4Ab4VMu13yBkh9VGOODQgkBirFvaKizmE0oUh8uIqejhM89huM8PxLcWxSYoOYdFE+DFTVpuq3QeRfl7fMg9QCNT8BBuPFIkKEJY1USVGBJI4b2Df8a0lbIugIAhLAtcPqckBeWGhkGxHeRpBDmMEuj9AD9IvvrBthOugfZek4BCVLEZVSViEcDLi/bOL3J1f4x5viTkTXOFABI04Rz3IIcOdpo23qatfyJ2eJbZFUBWI6SGuHVP0R3hWxJX+FEnBQZrmSx6A66x2YRhozyYspYmA7liP7y4/z5TRZs4cMm685MpPtKaV5Fjul1lvFbDbAtnsvab3KOMtRuvbdi6/1ajBALG0ijAM8uM5trdtNs0iy7ky9bKPLzsUhGZcGtc0XyAGYgwRAK+tRi2va6yiuLFCKCJhO0moK4dRYtGIcywNK6x1injrkuKVNMHfenGFZGPz9i+muG7eJlEInZflRAmB0Szi5vPY1QLDaoXYs1COSSuWDHMWnhNhGwmRZTCILHQsEbHA6gpy6wlWN8ba7KK63ew78q0QEqREGxJtCKShsUSM9UbEeoVMk/LLEBY1SV6Bk6A1KGPvGDuwSwaPtk3CkiCoaKr+CEsowMAVESVzyKTb5dTRBOv7jyITSGxQZtqILKxA7GsSX2HW+nheyOFKk/eULjBv1ynLAdtJQqg1v7X9QT5z+jiiaTG7/E0Snm8HpIEslxDFAuFshf684tH9l/GMiKI5xJExWguWPjxD7p53Yg40Xj3BCBL0jgqmNgWNYzadowkUQr5z/hLzZoOCjHB3jmmrkKeDKotRjc82jnPhG3P4q5LiYgLdLGcnYw+zI5jmrPeoPVclWPR5bvEIf3NhFtcLGc/32Z9v4BgxVatP1eyTN0Y87F7msJWgtKavFZGGkZY0lMtIW9TkgANWjC9sFuMhL4QTDJRDK/FpJz7dxOXJ+gKLjQpRaKLqNlZHYnUF1bMJ/towFccb3J4enW8HHUbQHyCFoHQphzW0SCxBlHdIHIfIgrN+FWVprJ6g0tRpgnkjxl8epAn7nV4W2n09aIXu9rA2LdB5krbNxWCStVEaupW+n3rlwugVnmPhOMhCHuE4tKYdhrMJ5CPoWZgbNlZfYLcD2EOfw64YPIlvEdQ00XjETL6NKxIMYeLLgDGzi/Q17fsvcHFfDaRmKt9l0u1Stgbcm1tmxmxiiwRfBtgofBlTlmAhaCjFSpJnPS7x2bNHmfoTC7ce4V5ukNyOoSxS7wy1CsFMke6cQ+3INv945o+vyegnWvDBwmn+bPwEa0GJ57enWTxXxey99PFrCwp3b/M/Hvkzpsw2C2aTGVNg8NKudyuR/H7zQb6xPcvGxTH2fzLGO7mMHgV3TE+mjFuQ67wK+sIVxjYbCNNAjZcJpvLEnkNzpszy3CyJDUklwi0FlHJDggWLycILRBq2lENXudSTPKeHszRjn7v8VcaMs7iG4sVojN/bfgfbozxb/Tztrkc0tCi8YDN5OsIYKeztNrLdhzhJqxjDMG15kIWBU7ROw3mjANHrY7faOI59rTAFIdIQmJSpRztRcDXfKY7RQYBWKlUTz0K73xqtSRotRLeHPRjH2drH8/0ZlvtlAESxAEGI6HTQwY3jKX0fPTVOkrfpzknG9m8x5vc5fWaO/KLA6Sjs+mhPpZDsjodHCpQJ0klwjZe8LqnxEpCYkgO5OrZMcIyYI/4mc3admtnjXnubacPbeUaa2h9pQUS6C+sqi6WoxnJYRbds/M0Qqz5IPRB7KFv8LceQJLZEWZC3Q2qGxrpOaM0WTUb587Q8H4A/b+YInZ3qBQEYmiPVbd7lXmbc0FhC7nTKVQQ63ulAXGSxX6HeymM1Jc5Gh3htfRfebEbGt4cajWCnO7wRhrhBjPZstCyQOAaJC4GyGClBHBlcmBxn2fMIMViPS3QTj+24wIXBGJ3Qw5Exy84SkR5yMTjMYrdKe+jS6XqojoXRN/A2Nd5iN9X02W4Qt9q7PAp7nJ2KLq2SVD8n20vdXFSCDhL0KMAIoBn69CM7LXgp5hAjCxnH18KtYsfoFPkcccklypskLrhmjGtEaXixnyq8yyBi75g7u2XwmJLEU7heRMEaYYk0Dj5mBBx31oi0wazVpOX7WCJh3OhQM/r4IqYgXuqrcrV/06moxGc6d7MRFHlma4bmlQpGTzL+AtgrbUR/iBrcvvLiOkkQnT7uhok2cly+Ms7/Nv5upu0W7/fPcdCEgtQcsur0zTaUwT4W0468a+ewZcwHymcpS4WBZCNRNBKbuiryp617Od2eYrVVJHm+RGFF428p5Hb79s85yLht0cMRotVB9i3yWmN3fBJbEuckkW+SOCZ/+uIj/EH1HaBBRgKhQERgDgUyhjP+IX6v/G60pTHbEm9LICOojDTmUGNEmvziAFlvoaNUBC8jY08SxzgNzdOL80ihiU7E9GdrmANBfnkar5GgTEHkSZQJYUkwnNIkrkYbCUuXxllOJii9aFA+P8DoBYhGe085GnbF4FGmQLuKoj+ivFMWCjBuOFSlBmIU2yRX1TcxMIQArJcayZHQVqlH5897x/nd0w+RNB2KZwyOfrmDbPWh3UO12t+80ejtglaoZgsRhHhK4V+s8vHy/cxW2szvq3PMalMSBr5IgIQj5iIf8i+/wvLOCYkjbCIS1hOfU6NZzg0n+YOT9+GfdXCamomnumk1VhwTZ3pGGbcwajBI1dsBsbGFbe0sh4axs4sVsKNCC7zUJkXpdD3Reqc6Ml2TroZV0h8UqLR5rg5D4ih+6TkZGXsQHcf4W4r+RY+opLj73kX+8tRTnB7O8F9evJ/miofyNLI8wnYiJoo9vnvsChVzwH889w6ML5VwG5rS+QHmyYuo4Yh4jzVt3R0dnlgjhgbtvselfo2n8xPX+jJxLQxzY/nn1fLOZMenMFA51uMSfeXwQmeapOmkYZamxtjuoBstVHCHSIprvdNDLESMIqwetJsea1KzFNVoJJtYQuALC0MILGHg7zw10TqtLEETaUVDhXSV5FwwxanBDBd7Y8imhdvQuE2dVmO1WntqEmdkfNvs5HlcC59kZNypKIU5VFhdA21KosTAlwElY0jOD2gVbYStsOwYy0wwpCLSBoE2CQOLUlfjdBRmL0wbje7B79OuGDzOlTqzfzZFUMxztniMf1g9hrLe2A1UqNTFjAK7AzOrCeYgxl0foFvtHWPnNq3KehV0HKOHGtloMf5sEW/boT9d4t/J97GyUGGfU+dDuTPMmTcqxbZVyEZi0dc2Xx0c5unOPhqBz+lLM7iLNuYApi8m+CtDjH4AzXZm7GRkZGTcZugwwr/UYiIsEpZNLul5/oftH8AwFJaZUJ1u0xu4BOs+8UCypMqsRLOIRFBa1tRe6GN00zBWvIcSla9nVwye+PIiudV18lcz7q/++0a4viGZ1juCa+m/yR2Yna/jGOKYJAwxnxxSec6hdHiOi7Uyf8TdHKltca+7xJx549i0lORsNEEjzvPJzROcuTCD7BqMPyuofaOBGIbQaKHaHdRVV35GRkZGxm2FjkLU+Ss4ixZepYxI5mhv5QkqGuveFveNr/Hs5gyjjTz+usZrKPIXu4jeENEbkDRbJHs8dLs7woNap+WDu/Litzk7OQMoheyNcJoFOlt5TieS388/xJJ/5dqhCZLlsMr5wQTtyOXiZg2rbmJ2BW47RnbT8lA1HGUCXhkZGRm3OTpJIAQ9GmF3YpymBASdeo7T9iStRp5iB+yuxurE14ydqxILe9XQucruGDwZNxW9I7JmbNaZ/qJP8KJD7BX4VPVRPuG+54ZjZQRGoBEKJjoKpxkigwRro4OuN9NQWdaLJiMjI+P2Ryt0rNC9Pu75TeytHMq3KV9wiXJjzA0U/koX2RulvSlbbVQYpekje9zYgczguT3Z0bFI6g3EV5q4AEKSZ6cFxfWHvlwFcydkldwCkzcjIyMj4y1kZ91XoxHqylKabgJ4gLcjCaO1umXvD5nBc7tzdWLqq9Uou3gtGRkZGRm3DtcbNvrWz43dO33bMzIyMjIyMjJuEpnBk5GRkZGRkXHbkxk8GRkZGRkZGbc9QutbNPsoIyMjIyMjI+N1knl4MjIyMjIyMm57MoMnIyMjIyMj47YnM3gyMjIyMjIybnsygycjIyMjIyPjticzeDIyMjIyMjJuezKDJyMjIyMjI+O25/8P5aRa/UAIBaQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizacion de algunas imagenes de los datos de entrenamiento\n",
    "\n",
    "chosen_idx = np.random.choice(training_images.shape[0], size=6, replace=False)\n",
    "\n",
    "fig, ax = plt.subplots(1, 6, figsize=(10,5))\n",
    "fig.set_facecolor('white')\n",
    "for i, idx in enumerate(chosen_idx):\n",
    "    image = training_images[idx, :]\n",
    "    digit = training_labels[idx]\n",
    "    ax[i].imshow(image.reshape((28, 28)))\n",
    "    ax[i].set_title(\"Etiqueta: %d\" % digit)\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q4f4FFyhpdDA"
   },
   "source": [
    "## Definición de Clasificador MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Bip-9FV1_WJ"
   },
   "source": [
    "### Función de costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CjgsGFG31-no"
   },
   "outputs": [],
   "source": [
    "def loss_fn(logits, labels, loss_function_name):\n",
    "    \"\"\"Calculo de la funcion de costo, donde se puede escoger entre 2 opciones; \n",
    "    xentropy o mse.\n",
    "    \n",
    "    Se aplica el loss 'loss_function_name' entre los labels reales y la salida\n",
    "    de la MLP. Ademas, se calcula el accuracy.\n",
    "    \n",
    "    Args:\n",
    "        logits: Tensor de dimensiones (batch_size, n_classes) con los logits\n",
    "        de la salida de la MLP\n",
    "        labels: Tensor de dimensiones (batch_size,) con las etiquetas reales.\n",
    "        loss_function_name: 'cross_entropy' o 'mse', selecciona el costo.\n",
    "    \n",
    "    Returns:\n",
    "        loss: Tensor escalar que corresponde al costo calculado.\n",
    "        accuracy: Tensor escalar que corresponde al accuracy calculado.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Codificacion 'one hot' para las etiquetas de clase\n",
    "    n_classes = logits.shape[1]\n",
    "    one_hot_labels = tf.one_hot(labels, n_classes)\n",
    "    with tf.name_scope('loss'):\n",
    "        if loss_function_name == 'cross_entropy':\n",
    "            # Cross Entropy loss\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=logits,\n",
    "                    labels=one_hot_labels),\n",
    "                name='xentropy'\n",
    "            )\n",
    "        elif loss_function_name == 'mse':\n",
    "            # Mean Squared Error loss\n",
    "            probabilities = tf.nn.softmax(logits)\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.square(one_hot_labels - probabilities),\n",
    "                name='mse'\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError('Wrong value for loss_function_name')\n",
    "    with tf.name_scope('accuracy'):\n",
    "        predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "        correct_predictions = tf.equal(labels, predictions)\n",
    "        accuracy = tf.reduce_mean(\n",
    "            tf.cast(correct_predictions, tf.float32),\n",
    "            name='accuracy')\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mx8DACFS8TxR"
   },
   "source": [
    "### Clase de Clasificador MLP\n",
    "Esta clase utiliza la funcion anteriores para entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fITl9UaSydAf"
   },
   "outputs": [],
   "source": [
    "class MLPClassifier(tf.keras.Model):\n",
    "    \"\"\"Implementacion de clasificador Perceptron Multicapa.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features,\n",
    "        layer_sizes,\n",
    "        loss_function_name='cross_entropy',\n",
    "        learning_rate=0.1,\n",
    "        batch_size=32,\n",
    "        max_epochs=100,\n",
    "        early_stopping=None,\n",
    "        logdir='logs'):\n",
    "        \"\"\"Construye un clasificador Perceptron Multicapa.\n",
    "        \n",
    "        Args:\n",
    "            n_features: Entero que indica el numero de caracteristicas de las entradas.\n",
    "            layer_sizes: Lista de enteros que indica el tamaño de cada capa de\n",
    "            neuronas. La salida de la capa i-esima posee dimensiones\n",
    "            (batch_size, layer_sizes[i]). El ultimo numero de la lista indica el\n",
    "            tamaño de la capa de salida, que debe ser igual al numero de clases.\n",
    "            loss_function_name: 'cross_entropy' o 'mse', selecciona el costo.\n",
    "                Por defecto es 'cross_entropy'.\n",
    "            learning_rate: Escalar que indica la tasa de aprendizaje. Al\n",
    "                seleccionar Adam este parametro es ignorado. Por defecto es 0.1\n",
    "            batch_size: Entero que indica el tamaño de los mini-batches para\n",
    "                el entrenamiento de la red.\n",
    "            max_epochs: Entero que indica el maximo numero de epocas de\n",
    "                entrenamiento (pasadas completas por los datos de entrada) \n",
    "            early_stopping: Indica cuantas veces las verificaciones en la\n",
    "                validacion deben indicar que el costo esta aumentando para\n",
    "                realizar una detencion temprana. Por defecto es None, lo cual\n",
    "                desactiva la detencion temprana.\n",
    "            logdir: String que indica el directorio en donde guardar los\n",
    "                archivos del entrenamiento. Por defecto es 'logs'.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Inicializa atributos propios de un objeto del tipo tf.keras.Model\n",
    "        super().__init__()\n",
    "        # Agregar parametros al objeto\n",
    "        self.n_features = n_features\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.loss_function_name = loss_function_name\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        self.early_stopping = early_stopping\n",
    "        self.logdir = logdir\n",
    "        # Inicializa capas del modelo\n",
    "        self.layers_list = self._init_layers(layer_sizes)\n",
    "        # Crea un objeto optimizador\n",
    "        self.optimizer = tf.keras.optimizers.SGD(learning_rate=self.learning_rate)\n",
    "        \n",
    "    def _init_layers(self, layer_sizes):\n",
    "        \"\"\"Inicializa-crea los objetos de cada capa de la red MLP.\n",
    "\n",
    "        Args:\n",
    "          layer_sizes: Lista de enteros que indica el tamaño de cada capa de\n",
    "              neuronas. La salida de la capa i-esima posee dimensiones\n",
    "              (batch_size, layer_sizes[i]). El ultimo numero de la lista indica el\n",
    "              tamaño de la capa de salida, que debe ser igual al numero de clases.\n",
    "\n",
    "        Returns:\n",
    "          layers_list: Lista de objetos asociados a cada capa creada.\n",
    "        \"\"\"\n",
    "        n_layers = len(layer_sizes)\n",
    "        layers_list = []\n",
    "        # Inicializacion de capas del modelo\n",
    "        layers_list.append(tf.keras.layers.InputLayer((self.n_features)))\n",
    "        for i in range(n_layers):\n",
    "            layers_list.append(tf.keras.layers.Dense(layer_sizes[i], name='dense_%i' % (i+1)))\n",
    "            if i < n_layers - 1:\n",
    "                # Inicializacion de funcion de activacion de capa oculta\n",
    "                layers_list.append(tf.keras.layers.Activation(tf.nn.sigmoid))\n",
    "            else:\n",
    "                # Inicializacion de funcion de activacion de capa de salida\n",
    "                layers_list.append(tf.keras.layers.Activation(tf.nn.softmax))\n",
    "        return layers_list\n",
    "\n",
    "    def call(self, x, training=False, get_logits=False):\n",
    "        \"\"\"Metodo que entrega la salida del modelo, \n",
    "        define el paso forward de la red neuronal, relacionando las capas \n",
    "        de la red MLP entre si. Se define la arquitectura del modelo.\n",
    "\n",
    "        Args:\n",
    "          x: Tensor de entrada de dimensiones (batch_size, n_features).\n",
    "          training: Boolean que indica si se esta en fase de entrenamiento o no.\n",
    "            En este caso es irrelevante, pero resulta util al definir capas que \n",
    "            tienen diferentes comportamientos durante entrenamiento y evaluacion\n",
    "             como dropout o batch normalization \n",
    "          get_logits: Bolean que indica si como salida del modelo se obtienen los\n",
    "            logits o las predicciones\n",
    "\n",
    "        Returns:\n",
    "          x: Tensor de salida de la red, de dimensiones (batch_size, n_classes).\n",
    "        \"\"\"\n",
    "        for layer_index, layer in enumerate(self.layers_list):\n",
    "            # se se desean los logits, se retorna la salida del modelo sin la ultima activacion\n",
    "            if layer_index == (len(self.layers_list)-1) and get_logits==True:\n",
    "                return x \n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, x_data, y_labels):\n",
    "        \"\"\"Metodo que realiza la actualizacion por gradiente.\n",
    "\n",
    "        Se aplica el algoritmo de optimizacion 'sgd' para ejecutar una\n",
    "        iteracion de minimizacion por gradiente sobre el loss del modelo.\n",
    "\n",
    "        Args:\n",
    "          x_data: datos de entrenamientos sobre los que calcular la loss.\n",
    "          y_labels: etiquetas de entrenamiento sobre las que calcular la loss.\n",
    "\n",
    "        Returns:\n",
    "          loss: el valor de la funcion de costo para x_data, y_labels.\n",
    "          accuracy: porcentaje de aciertos entre y_labels y las predicciones sobre x_data.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # training=True is only needed if there are layers with different\n",
    "            # behavior during training versus inference (e.g. Dropout).\n",
    "            logits = self.call(x_data, training=True, get_logits=True)\n",
    "            loss, accuracy = loss_fn(logits, y_labels, self.loss_function_name)\n",
    "            gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return loss, accuracy\n",
    "\n",
    "    @tf.function\n",
    "    def eval_step(self, x_data, y_labels):\n",
    "        \"\"\"Metodo que evalua la funcion de costo y el accuracy del modelo.\n",
    "\n",
    "        Args:\n",
    "          x_data: datos sobre los que calcular la loss y accuracy.\n",
    "          y_labels: etiquetas sobre las que calcular la loss y accuracy.\n",
    "\n",
    "        Returns:\n",
    "          loss: el valor de la funcion de costo para x_data, y_labels.\n",
    "          accuracy: porcentaje de aciertos entre y_labels y las predicciones sobre x_data.\n",
    "        \"\"\"\n",
    "        logits = self.call(x_data, training=False, get_logits=True)\n",
    "        loss, accuracy = loss_fn(logits, y_labels, self.loss_function_name)\n",
    "        return loss, accuracy\n",
    "\n",
    "    @tf.function\n",
    "    def prediction_step(self, x_data):\n",
    "        \"\"\"Metodo que retorna predicciones para x_data\n",
    "\n",
    "        Args:\n",
    "          x_data: datos a predecir.\n",
    "\n",
    "        Returns:\n",
    "          predictions: predicciones para x_data\n",
    "        \"\"\"\n",
    "        predictions = self.call(x_data, training=False, get_logits=False)\n",
    "        return predictions\n",
    "\n",
    "    def write_to_train_summary(self, train_summary_writer, step, loss, accuracy):\n",
    "        \"\"\"Metodo que calcula metricas y parametros a ser monitoreados en tensorboard.\n",
    "        Monitorea histograma de parametros y metricas del modelo. Para datos de entrenamiento\n",
    "\n",
    "        Args:\n",
    "          train_summary_writer: Summary writer de entrenamiento\n",
    "          step: iteracion del entrenamiento\n",
    "          loss: valor de la funcion de costo para datos de entrenamiento\n",
    "          accuracy: accuracy para datos de entrenamiento\n",
    "        \"\"\"\n",
    "        with train_summary_writer.as_default():\n",
    "            for layer_i in self.layers_list:\n",
    "                layer_name = layer_i.name\n",
    "                if 'dense' in layer_name:\n",
    "                    weights, biases = layer_i.get_weights()\n",
    "                    tf.summary.histogram(layer_name + '_weights', weights, step=step)\n",
    "                    tf.summary.histogram(layer_name + '_biases', biases, step=step)\n",
    "            tf.summary.scalar(self.loss_function_name + '_loss', loss, step=step)  \n",
    "            tf.summary.scalar('accuracy', accuracy, step=step)  \n",
    "\n",
    "    def write_to_val_summary(self, val_summary_writer, step, loss, accuracy):\n",
    "        \"\"\"Metodo que calcula metricas a ser monitoreados en tensorboard.\n",
    "        Monitorea metricas del modelo. Para datos de validacion\n",
    "\n",
    "        Args:\n",
    "          validacion_summary_writer: Summary writer de validacion\n",
    "          step: iteracion del entrenamiento\n",
    "          loss: valor de la funcion de costo para datos de validacion\n",
    "          accuracy: accuracy para datos de validacion\n",
    "        \"\"\"\n",
    "        with val_summary_writer.as_default():\n",
    "            tf.summary.scalar(self.loss_function_name + '_loss', loss, step=step)  \n",
    "            tf.summary.scalar('accuracy', accuracy, step=step)  \n",
    "    \n",
    "        \n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Entrenamiento del clasificador con los hiperparametros escogidos.\n",
    "        \n",
    "        Args:\n",
    "            X_train: Entradas del entrenamiento con dimensiones (n_ejemplos, n_features).\n",
    "            y_train: Etiquetas del entrenamiento con dimensiones (n_ejemplos,)\n",
    "            X_train: Entradas de la validacion con dimensiones (n_ejemplos, n_features).\n",
    "            y_train: Etiquetas de la validacion con dimensiones (n_ejemplos,)\n",
    "            \n",
    "        Returns:\n",
    "            train_stats: Diccionario con datos historicos del entrenamiento.\n",
    "        \"\"\"\n",
    "        # Creacion de 'writers' que guardan datos para Tensorboard\n",
    "        train_summary_writer = tf.summary.create_file_writer(\n",
    "            self.logdir + '/train')\n",
    "        val_summary_writer = tf.summary.create_file_writer(self.logdir + '/val')\n",
    "        print('\\n\\n[Beginning training of MLP at logdir \"%s\"]\\n' % (self.logdir,))    \n",
    "        # Definicion de variables utiles para el entrenamiento\n",
    "        n_batches = int(X_train.shape[0] / self.batch_size)\n",
    "        prev_validation_loss = 100.0\n",
    "        validation_period = 100\n",
    "        early_stop_flag = False\n",
    "        start_time = time.time()\n",
    "        iteration_history = []\n",
    "        train_loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_loss_history = []\n",
    "        val_acc_history = []\n",
    "        # Se crea dataset para iterar sobre los batch de los datos de entreneminto. \n",
    "        # Para cada nueva epoca, se hacer un shuffle al set de train\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (X_train, y_train)).shuffle(X_train.shape[0]).batch(self.batch_size)\n",
    "        # Ciclo que recorre una epoca completa de los datos cada vez\n",
    "        for epoch in range(self.max_epochs):\n",
    "            if early_stop_flag:\n",
    "                # Si early stopping se activo, detener el entrenamiento\n",
    "                break            \n",
    "            # Ciclo que recorre los mini batches del set de train\n",
    "            for i, (X_batch, y_batch) in enumerate(train_dataset):\n",
    "                if early_stop_flag:\n",
    "                    # Si early stopping se activo, detener el entrenamiento\n",
    "                    break  \n",
    "                iteration = epoch * n_batches + i                    \n",
    "                # Ejecutar una iteracion de gradiente\n",
    "                self.train_step(X_batch, y_batch)\n",
    "                # Obtener estadisticas del entrenamiento\n",
    "                if iteration % validation_period == 0:\n",
    "                    iteration_history.append(iteration)\n",
    "                    # Estadisticas en el set de validacion\n",
    "                    val_loss, val_acc = self.eval_step(X_val, y_val)\n",
    "                    # Escribir estadisticas de validacion en tensorboard\n",
    "                    self.write_to_val_summary(\n",
    "                        val_summary_writer, iteration, val_loss, val_acc)\n",
    "                    val_loss_history.append(val_loss)\n",
    "                    val_acc_history.append(val_acc)\n",
    "                    # Estadisticas en el set de entrenamiento\n",
    "                    train_loss, train_acc = self.eval_step(X_train, y_train)\n",
    "                    # Escribir estadisticas e histogramas de parametros de \n",
    "                    # entrenamiento en tensorboard\n",
    "                    self.write_to_train_summary(\n",
    "                        train_summary_writer, iteration, train_loss, train_acc)\n",
    "                    train_loss_history.append(train_loss)\n",
    "                    train_acc_history.append(train_acc)\n",
    "                    \n",
    "                    print('Epoch: %d/%d, iter: %d. ' %\n",
    "                          (epoch+1, self.max_epochs, iteration), end='')\n",
    "                    print('Loss (train/val): %.3f / %.3f. Val. acc: %.1f%%' %\n",
    "                          (train_loss, val_loss, val_acc * 100), end='')\n",
    "                    \n",
    "                    # Chequear condicion de early_stopping\n",
    "                    if self.early_stopping is not None:\n",
    "                        if val_loss > prev_validation_loss:\n",
    "                            validation_checks += 1\n",
    "                        else:\n",
    "                            validation_checks = 0\n",
    "                            prev_validation_loss = val_loss\n",
    "                        print(', Val. checks: %d/%d' %\n",
    "                              (validation_checks, self.early_stopping))\n",
    "                        if validation_checks >= self.early_stopping:\n",
    "                            early_stop_flag = True\n",
    "                            print('Early stopping')\n",
    "                    else:\n",
    "                        print('')\n",
    "            elap_time = time.time()-start_time\n",
    "            print(\"Epoch finished. Elapsed time %1.4f [s]\\n\" % (elap_time,))\n",
    "        # Guardar estadisticas en un diccionario\n",
    "        train_stats = {\n",
    "            'iteration_history': np.array(iteration_history),\n",
    "            'train_loss_history': np.array(train_loss_history),\n",
    "            'train_acc_history': np.array(train_acc_history),\n",
    "            'val_loss_history': np.array(val_loss_history),\n",
    "            'val_acc_history': np.array(val_acc_history)\n",
    "        }\n",
    "        # Guardar grafo de evaluacion en tensorboard\n",
    "        self.write_graph_to_summary(train_summary_writer, X_train)\n",
    "        return train_stats\n",
    "\n",
    "    def write_graph_to_summary(self, train_summary_writer, X_train):\n",
    "        \"\"\"Metodo que guarda el grafo de evaluacion (arquitectura del modelo)\n",
    "        en tensorboard. Se guardan las operaciones involugradas en un passo forward\n",
    "        o self.call del modelo\n",
    "\n",
    "        Args:\n",
    "          train_summary_writer: Summary writer para datos de entrenamiento.\n",
    "          X_train: Datos de entrenamiento\n",
    "        \"\"\"\n",
    "        logdir = self.logdir + '/graph'\n",
    "        tf.summary.trace_on(graph=True, profiler=False)\n",
    "        predicted_proba = self.prediction_step(X_train)\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.trace_export(\n",
    "                name=\"prediction_step\",\n",
    "                step=0,\n",
    "                profiler_outdir=logdir)\n",
    "            tf.summary.trace_off()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Retorna las probabilidades de clase para los datos de entrada.\n",
    "              \n",
    "        Args:\n",
    "            X: datos de entrada\n",
    "        \n",
    "        Returns:\n",
    "            predicted_proba.numpy(): Arreglo numpy de las probabilidades de \n",
    "              cada clase, para cada muestra de X.\n",
    "        \"\"\"\n",
    "        # Obtener las probabilidades de salida de cada clase\n",
    "        predicted_proba = self.prediction_step(X)\n",
    "        return predicted_proba.numpy()\n",
    "    \n",
    "    def predict_label(self, X):\n",
    "        \"\"\"Retorna la etiqueta predicha para los datos de entrada.\n",
    "        \n",
    "        Args:\n",
    "            X: datos de entrada\n",
    "        \n",
    "        Returns:\n",
    "            predicted_labels: Arreglo numpy de las predicciones de cada muestra de X.\n",
    "        \"\"\"\n",
    "        # Obtener la probabilidad de cada clase\n",
    "        predicted_proba = self.prediction_step(X)\n",
    "        # Etiquetar segun la etiqueta mas probable\n",
    "        predicted_labels = np.argmax(predicted_proba, axis=1)\n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HUxx8YiRbDl"
   },
   "source": [
    "## Entrenamiento de MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aYshAaUvRaZa"
   },
   "outputs": [],
   "source": [
    "# ----- Directorio para logs\n",
    "experiment_name = \"experiment_14\"\n",
    "\n",
    "# --- NO TOCAR\n",
    "logdir_father = \"./tarea_1_logs/\"\n",
    "logdir = logdir_father + experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xmNz6vshn1q2",
    "outputId": "71f464ee-7d8a-4c99-e975-8a05992a97f0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Beginning training of MLP at logdir \"./tarea_1_logs/experiment_14/run_0\"]\n",
      "\n",
      "Epoch: 1/100, iter: 0. Loss (train/val): 8.084 / 8.002. Val. acc: 50.0%, Val. checks: 0/15\n",
      "Epoch: 1/100, iter: 100. Loss (train/val): 2.201 / 2.201. Val. acc: 50.0%, Val. checks: 0/15\n",
      "Epoch: 1/100, iter: 200. Loss (train/val): 0.815 / 0.815. Val. acc: 50.0%, Val. checks: 0/15\n",
      "Epoch: 1/100, iter: 300. Loss (train/val): 0.746 / 0.746. Val. acc: 50.0%, Val. checks: 0/15\n",
      "Epoch finished. Elapsed time 1.8139 [s]\n",
      "\n",
      "Epoch: 2/100, iter: 400. Loss (train/val): 2.229 / 2.229. Val. acc: 50.0%, Val. checks: 1/15\n",
      "Epoch: 2/100, iter: 500. Loss (train/val): 3.467 / 3.467. Val. acc: 50.0%, Val. checks: 2/15\n",
      "Epoch: 2/100, iter: 600. Loss (train/val): 3.267 / 3.267. Val. acc: 50.0%, Val. checks: 3/15\n",
      "Epoch finished. Elapsed time 2.4298 [s]\n",
      "\n",
      "Epoch: 3/100, iter: 700. Loss (train/val): 3.451 / 3.451. Val. acc: 50.0%, Val. checks: 4/15\n",
      "Epoch: 3/100, iter: 800. Loss (train/val): 3.391 / 3.391. Val. acc: 50.0%, Val. checks: 5/15\n",
      "Epoch: 3/100, iter: 900. Loss (train/val): 1.244 / 1.244. Val. acc: 50.0%, Val. checks: 6/15\n",
      "Epoch: 3/100, iter: 1000. Loss (train/val): 1.361 / 1.361. Val. acc: 50.0%, Val. checks: 7/15\n",
      "Epoch finished. Elapsed time 3.0745 [s]\n",
      "\n",
      "Epoch: 4/100, iter: 1100. Loss (train/val): 3.640 / 3.640. Val. acc: 50.0%, Val. checks: 8/15\n",
      "Epoch: 4/100, iter: 1200. Loss (train/val): 1.462 / 1.462. Val. acc: 50.0%, Val. checks: 9/15\n",
      "Epoch: 4/100, iter: 1300. Loss (train/val): 0.959 / 0.959. Val. acc: 50.0%, Val. checks: 10/15\n",
      "Epoch finished. Elapsed time 3.7217 [s]\n",
      "\n",
      "Epoch: 5/100, iter: 1400. Loss (train/val): 3.639 / 3.639. Val. acc: 50.0%, Val. checks: 11/15\n",
      "Epoch: 5/100, iter: 1500. Loss (train/val): 1.305 / 1.305. Val. acc: 50.0%, Val. checks: 12/15\n",
      "Epoch: 5/100, iter: 1600. Loss (train/val): 0.782 / 0.782. Val. acc: 50.0%, Val. checks: 13/15\n",
      "Epoch finished. Elapsed time 4.3203 [s]\n",
      "\n",
      "Epoch: 6/100, iter: 1700. Loss (train/val): 2.027 / 2.027. Val. acc: 50.0%, Val. checks: 14/15\n",
      "Epoch: 6/100, iter: 1800. Loss (train/val): 3.353 / 3.353. Val. acc: 50.0%, Val. checks: 15/15\n",
      "Early stopping\n",
      "Epoch finished. Elapsed time 4.6034 [s]\n",
      "\n",
      "\n",
      "\n",
      "[Beginning training of MLP at logdir \"./tarea_1_logs/experiment_14/run_1\"]\n",
      "\n",
      "Epoch: 1/100, iter: 0. Loss (train/val): 0.758 / 0.687. Val. acc: 68.9%, Val. checks: 0/15\n",
      "Epoch: 1/100, iter: 100. Loss (train/val): 5.359 / 5.360. Val. acc: 50.0%, Val. checks: 1/15\n",
      "Epoch: 1/100, iter: 200. Loss (train/val): 10.065 / 10.074. Val. acc: 50.0%, Val. checks: 2/15\n",
      "Epoch: 1/100, iter: 300. Loss (train/val): 1.079 / 0.991. Val. acc: 50.0%, Val. checks: 3/15\n",
      "Epoch finished. Elapsed time 0.9816 [s]\n",
      "\n",
      "Epoch: 2/100, iter: 400. Loss (train/val): 1.860 / 1.479. Val. acc: 86.0%, Val. checks: 4/15\n",
      "Epoch: 2/100, iter: 500. Loss (train/val): 1.381 / 1.232. Val. acc: 84.2%, Val. checks: 5/15\n",
      "Epoch: 2/100, iter: 600. Loss (train/val): 2.094 / 1.933. Val. acc: 83.1%, Val. checks: 6/15\n",
      "Epoch finished. Elapsed time 1.5778 [s]\n",
      "\n",
      "Epoch: 3/100, iter: 700. Loss (train/val): 1.638 / 1.599. Val. acc: 50.0%, Val. checks: 7/15\n",
      "Epoch: 3/100, iter: 800. Loss (train/val): 1.796 / 1.574. Val. acc: 50.0%, Val. checks: 8/15\n",
      "Epoch: 3/100, iter: 900. Loss (train/val): 1.774 / 1.418. Val. acc: 85.3%, Val. checks: 9/15\n",
      "Epoch: 3/100, iter: 1000. Loss (train/val): 1.604 / 1.244. Val. acc: 83.8%, Val. checks: 10/15\n",
      "Epoch finished. Elapsed time 2.2173 [s]\n",
      "\n",
      "Epoch: 4/100, iter: 1100. Loss (train/val): 1.044 / 0.996. Val. acc: 87.0%, Val. checks: 11/15\n",
      "Epoch: 4/100, iter: 1200. Loss (train/val): 1.000 / 0.814. Val. acc: 87.8%, Val. checks: 12/15\n",
      "Epoch: 4/100, iter: 1300. Loss (train/val): 0.917 / 0.834. Val. acc: 86.5%, Val. checks: 13/15\n",
      "Epoch finished. Elapsed time 2.8349 [s]\n",
      "\n",
      "Epoch: 5/100, iter: 1400. Loss (train/val): 2.656 / 2.563. Val. acc: 63.5%, Val. checks: 14/15\n",
      "Epoch: 5/100, iter: 1500. Loss (train/val): 0.928 / 0.783. Val. acc: 87.2%, Val. checks: 15/15\n",
      "Early stopping\n",
      "Epoch finished. Elapsed time 3.1603 [s]\n",
      "\n",
      "\n",
      "\n",
      "[Beginning training of MLP at logdir \"./tarea_1_logs/experiment_14/run_2\"]\n",
      "\n",
      "Epoch: 1/100, iter: 0. Loss (train/val): 12.076 / 12.071. Val. acc: 50.0%, Val. checks: 0/15\n",
      "Epoch: 1/100, iter: 100. Loss (train/val): 2.332 / 2.333. Val. acc: 50.0%, Val. checks: 0/15\n",
      "Epoch: 1/100, iter: 200. Loss (train/val): 2.937 / 2.937. Val. acc: 50.0%, Val. checks: 1/15\n",
      "Epoch: 1/100, iter: 300. Loss (train/val): 1.910 / 1.910. Val. acc: 50.0%, Val. checks: 0/15\n",
      "Epoch finished. Elapsed time 0.9616 [s]\n",
      "\n",
      "Epoch: 2/100, iter: 400. Loss (train/val): 1.469 / 1.469. Val. acc: 50.0%, Val. checks: 0/15\n",
      "Epoch: 2/100, iter: 500. Loss (train/val): 2.025 / 2.025. Val. acc: 50.0%, Val. checks: 1/15\n",
      "Epoch: 2/100, iter: 600. Loss (train/val): 1.736 / 1.736. Val. acc: 50.0%, Val. checks: 2/15\n",
      "Epoch finished. Elapsed time 1.5541 [s]\n",
      "\n",
      "Epoch: 3/100, iter: 700. Loss (train/val): 1.703 / 1.703. Val. acc: 50.0%, Val. checks: 3/15\n",
      "Epoch: 3/100, iter: 800. Loss (train/val): 3.655 / 3.655. Val. acc: 50.0%, Val. checks: 4/15\n",
      "Epoch: 3/100, iter: 900. Loss (train/val): 1.626 / 1.626. Val. acc: 50.0%, Val. checks: 5/15\n",
      "Epoch: 3/100, iter: 1000. Loss (train/val): 2.073 / 2.073. Val. acc: 50.0%, Val. checks: 6/15\n",
      "Epoch finished. Elapsed time 2.1810 [s]\n",
      "\n",
      "Epoch: 4/100, iter: 1100. Loss (train/val): 1.630 / 1.629. Val. acc: 50.0%, Val. checks: 7/15\n",
      "Epoch: 4/100, iter: 1200. Loss (train/val): 3.265 / 3.265. Val. acc: 50.0%, Val. checks: 8/15\n",
      "Epoch: 4/100, iter: 1300. Loss (train/val): 1.427 / 1.427. Val. acc: 50.0%, Val. checks: 0/15\n",
      "Epoch finished. Elapsed time 2.7691 [s]\n",
      "\n",
      "Epoch: 5/100, iter: 1400. Loss (train/val): 1.890 / 1.890. Val. acc: 50.0%, Val. checks: 1/15\n",
      "Epoch: 5/100, iter: 1500. Loss (train/val): 2.154 / 2.154. Val. acc: 50.0%, Val. checks: 2/15\n",
      "Epoch: 5/100, iter: 1600. Loss (train/val): 1.977 / 1.977. Val. acc: 50.0%, Val. checks: 3/15\n",
      "Epoch finished. Elapsed time 3.3638 [s]\n",
      "\n",
      "Epoch: 6/100, iter: 1700. Loss (train/val): 1.740 / 1.740. Val. acc: 50.0%, Val. checks: 4/15\n",
      "Epoch: 6/100, iter: 1800. Loss (train/val): 1.806 / 1.806. Val. acc: 50.0%, Val. checks: 5/15\n",
      "Epoch: 6/100, iter: 1900. Loss (train/val): 3.189 / 3.189. Val. acc: 50.0%, Val. checks: 6/15\n",
      "Epoch: 6/100, iter: 2000. Loss (train/val): 2.410 / 2.410. Val. acc: 50.0%, Val. checks: 7/15\n",
      "Epoch finished. Elapsed time 3.9956 [s]\n",
      "\n",
      "Epoch: 7/100, iter: 2100. Loss (train/val): 3.461 / 3.461. Val. acc: 50.0%, Val. checks: 8/15\n",
      "Epoch: 7/100, iter: 2200. Loss (train/val): 1.302 / 1.302. Val. acc: 50.0%, Val. checks: 0/15\n",
      "Epoch: 7/100, iter: 2300. Loss (train/val): 2.809 / 2.809. Val. acc: 50.0%, Val. checks: 1/15\n",
      "Epoch finished. Elapsed time 4.5870 [s]\n",
      "\n",
      "Epoch: 8/100, iter: 2400. Loss (train/val): 3.887 / 3.887. Val. acc: 50.0%, Val. checks: 2/15\n",
      "Epoch: 8/100, iter: 2500. Loss (train/val): 2.582 / 2.582. Val. acc: 50.0%, Val. checks: 3/15\n",
      "Epoch: 8/100, iter: 2600. Loss (train/val): 3.024 / 3.024. Val. acc: 50.0%, Val. checks: 4/15\n",
      "Epoch finished. Elapsed time 5.2068 [s]\n",
      "\n",
      "Epoch: 9/100, iter: 2700. Loss (train/val): 1.077 / 1.077. Val. acc: 50.0%, Val. checks: 0/15\n",
      "Epoch: 9/100, iter: 2800. Loss (train/val): 1.799 / 1.799. Val. acc: 50.0%, Val. checks: 1/15\n",
      "Epoch: 9/100, iter: 2900. Loss (train/val): 3.027 / 3.027. Val. acc: 50.0%, Val. checks: 2/15\n",
      "Epoch: 9/100, iter: 3000. Loss (train/val): 0.713 / 0.713. Val. acc: 50.0%, Val. checks: 0/15\n",
      "Epoch finished. Elapsed time 5.8460 [s]\n",
      "\n",
      "Epoch: 10/100, iter: 3100. Loss (train/val): 3.461 / 3.461. Val. acc: 50.0%, Val. checks: 1/15\n",
      "Epoch: 10/100, iter: 3200. Loss (train/val): 3.328 / 3.328. Val. acc: 50.0%, Val. checks: 2/15\n",
      "Epoch: 10/100, iter: 3300. Loss (train/val): 4.181 / 4.181. Val. acc: 50.0%, Val. checks: 3/15\n",
      "Epoch finished. Elapsed time 6.4855 [s]\n",
      "\n",
      "Epoch: 11/100, iter: 3400. Loss (train/val): 3.234 / 3.234. Val. acc: 50.0%, Val. checks: 4/15\n",
      "Epoch: 11/100, iter: 3500. Loss (train/val): 2.645 / 2.645. Val. acc: 50.0%, Val. checks: 5/15\n",
      "Epoch: 11/100, iter: 3600. Loss (train/val): 3.192 / 3.192. Val. acc: 50.0%, Val. checks: 6/15\n",
      "Epoch finished. Elapsed time 7.1087 [s]\n",
      "\n",
      "Epoch: 12/100, iter: 3700. Loss (train/val): 1.454 / 1.454. Val. acc: 50.0%, Val. checks: 7/15\n",
      "Epoch: 12/100, iter: 3800. Loss (train/val): 0.709 / 0.709. Val. acc: 50.0%, Val. checks: 0/15\n",
      "Epoch: 12/100, iter: 3900. Loss (train/val): 2.218 / 2.218. Val. acc: 50.0%, Val. checks: 1/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/100, iter: 4000. Loss (train/val): 1.629 / 1.629. Val. acc: 50.0%, Val. checks: 2/15\n",
      "Epoch finished. Elapsed time 7.7771 [s]\n",
      "\n",
      "Epoch: 13/100, iter: 4100. Loss (train/val): 1.614 / 1.614. Val. acc: 50.0%, Val. checks: 3/15\n",
      "Epoch: 13/100, iter: 4200. Loss (train/val): 1.721 / 1.721. Val. acc: 50.0%, Val. checks: 4/15\n",
      "Epoch: 13/100, iter: 4300. Loss (train/val): 1.104 / 1.104. Val. acc: 50.0%, Val. checks: 5/15\n",
      "Epoch finished. Elapsed time 8.4095 [s]\n",
      "\n",
      "Epoch: 14/100, iter: 4400. Loss (train/val): 1.696 / 1.696. Val. acc: 50.0%, Val. checks: 6/15\n",
      "Epoch: 14/100, iter: 4500. Loss (train/val): 2.645 / 2.645. Val. acc: 50.0%, Val. checks: 7/15\n",
      "Epoch: 14/100, iter: 4600. Loss (train/val): 2.230 / 2.230. Val. acc: 50.0%, Val. checks: 8/15\n",
      "Epoch finished. Elapsed time 8.9999 [s]\n",
      "\n",
      "Epoch: 15/100, iter: 4700. Loss (train/val): 1.829 / 1.829. Val. acc: 50.0%, Val. checks: 9/15\n",
      "Epoch: 15/100, iter: 4800. Loss (train/val): 3.653 / 3.653. Val. acc: 50.0%, Val. checks: 10/15\n",
      "Epoch: 15/100, iter: 4900. Loss (train/val): 1.928 / 1.928. Val. acc: 50.0%, Val. checks: 11/15\n",
      "Epoch: 15/100, iter: 5000. Loss (train/val): 2.108 / 2.108. Val. acc: 50.0%, Val. checks: 12/15\n",
      "Epoch finished. Elapsed time 9.6772 [s]\n",
      "\n",
      "Epoch: 16/100, iter: 5100. Loss (train/val): 0.701 / 0.701. Val. acc: 50.0%, Val. checks: 0/15\n",
      "Epoch: 16/100, iter: 5200. Loss (train/val): 2.411 / 2.411. Val. acc: 50.0%, Val. checks: 1/15\n",
      "Epoch: 16/100, iter: 5300. Loss (train/val): 2.846 / 2.846. Val. acc: 50.0%, Val. checks: 2/15\n",
      "Epoch finished. Elapsed time 10.2678 [s]\n",
      "\n",
      "Epoch: 17/100, iter: 5400. Loss (train/val): 2.116 / 2.116. Val. acc: 50.0%, Val. checks: 3/15\n",
      "Epoch: 17/100, iter: 5500. Loss (train/val): 2.900 / 2.900. Val. acc: 50.0%, Val. checks: 4/15\n",
      "Epoch: 17/100, iter: 5600. Loss (train/val): 2.713 / 2.713. Val. acc: 50.0%, Val. checks: 5/15\n",
      "Epoch finished. Elapsed time 10.9008 [s]\n",
      "\n",
      "Epoch: 18/100, iter: 5700. Loss (train/val): 5.214 / 5.214. Val. acc: 50.0%, Val. checks: 6/15\n",
      "Epoch: 18/100, iter: 5800. Loss (train/val): 2.221 / 2.221. Val. acc: 50.0%, Val. checks: 7/15\n",
      "Epoch: 18/100, iter: 5900. Loss (train/val): 4.464 / 4.464. Val. acc: 50.0%, Val. checks: 8/15\n",
      "Epoch: 18/100, iter: 6000. Loss (train/val): 2.397 / 2.397. Val. acc: 50.0%, Val. checks: 9/15\n",
      "Epoch finished. Elapsed time 11.5441 [s]\n",
      "\n",
      "Epoch: 19/100, iter: 6100. Loss (train/val): 2.983 / 2.983. Val. acc: 50.0%, Val. checks: 10/15\n",
      "Epoch: 19/100, iter: 6200. Loss (train/val): 2.708 / 2.708. Val. acc: 50.0%, Val. checks: 11/15\n",
      "Epoch: 19/100, iter: 6300. Loss (train/val): 1.646 / 1.646. Val. acc: 50.0%, Val. checks: 12/15\n",
      "Epoch finished. Elapsed time 12.1381 [s]\n",
      "\n",
      "Epoch: 20/100, iter: 6400. Loss (train/val): 1.285 / 1.285. Val. acc: 50.0%, Val. checks: 13/15\n",
      "Epoch: 20/100, iter: 6500. Loss (train/val): 1.712 / 1.712. Val. acc: 50.0%, Val. checks: 14/15\n",
      "Epoch: 20/100, iter: 6600. Loss (train/val): 2.250 / 2.250. Val. acc: 50.0%, Val. checks: 15/15\n",
      "Early stopping\n",
      "Epoch finished. Elapsed time 12.6285 [s]\n",
      "\n",
      "\n",
      "\n",
      "[Beginning training of MLP at logdir \"./tarea_1_logs/experiment_14/run_3\"]\n",
      "\n",
      "Epoch: 1/100, iter: 0. Loss (train/val): 4.232 / 4.230. Val. acc: 50.0%, Val. checks: 0/15\n",
      "Epoch: 1/100, iter: 100. Loss (train/val): 1.419 / 1.414. Val. acc: 50.0%, Val. checks: 0/15\n",
      "Epoch: 1/100, iter: 200. Loss (train/val): 1.411 / 1.310. Val. acc: 81.2%, Val. checks: 0/15\n",
      "Epoch: 1/100, iter: 300. Loss (train/val): 0.528 / 0.508. Val. acc: 78.0%, Val. checks: 0/15\n",
      "Epoch finished. Elapsed time 0.9723 [s]\n",
      "\n",
      "Epoch: 2/100, iter: 400. Loss (train/val): 0.841 / 0.705. Val. acc: 83.4%, Val. checks: 1/15\n",
      "Epoch: 2/100, iter: 500. Loss (train/val): 0.436 / 0.397. Val. acc: 87.6%, Val. checks: 0/15\n",
      "Epoch: 2/100, iter: 600. Loss (train/val): 3.873 / 3.474. Val. acc: 50.0%, Val. checks: 1/15\n",
      "Epoch finished. Elapsed time 1.5649 [s]\n",
      "\n",
      "Epoch: 3/100, iter: 700. Loss (train/val): 0.713 / 0.718. Val. acc: 50.0%, Val. checks: 2/15\n",
      "Epoch: 3/100, iter: 800. Loss (train/val): 0.383 / 0.323. Val. acc: 90.8%, Val. checks: 0/15\n",
      "Epoch: 3/100, iter: 900. Loss (train/val): 0.386 / 0.338. Val. acc: 89.6%, Val. checks: 1/15\n",
      "Epoch: 3/100, iter: 1000. Loss (train/val): 0.448 / 0.448. Val. acc: 83.0%, Val. checks: 2/15\n",
      "Epoch finished. Elapsed time 2.2022 [s]\n",
      "\n",
      "Epoch: 4/100, iter: 1100. Loss (train/val): 0.434 / 0.396. Val. acc: 91.1%, Val. checks: 3/15\n",
      "Epoch: 4/100, iter: 1200. Loss (train/val): 0.397 / 0.374. Val. acc: 88.9%, Val. checks: 4/15\n",
      "Epoch: 4/100, iter: 1300. Loss (train/val): 0.351 / 0.343. Val. acc: 89.1%, Val. checks: 5/15\n",
      "Epoch finished. Elapsed time 2.8251 [s]\n",
      "\n",
      "Epoch: 5/100, iter: 1400. Loss (train/val): 0.401 / 0.407. Val. acc: 85.9%, Val. checks: 6/15\n",
      "Epoch: 5/100, iter: 1500. Loss (train/val): 0.332 / 0.325. Val. acc: 90.8%, Val. checks: 7/15\n",
      "Epoch: 5/100, iter: 1600. Loss (train/val): 0.352 / 0.327. Val. acc: 91.9%, Val. checks: 8/15\n",
      "Epoch finished. Elapsed time 3.4168 [s]\n",
      "\n",
      "Epoch: 6/100, iter: 1700. Loss (train/val): 0.690 / 0.651. Val. acc: 83.9%, Val. checks: 9/15\n",
      "Epoch: 6/100, iter: 1800. Loss (train/val): 0.395 / 0.378. Val. acc: 87.4%, Val. checks: 10/15\n",
      "Epoch: 6/100, iter: 1900. Loss (train/val): 0.271 / 0.252. Val. acc: 92.7%, Val. checks: 0/15\n",
      "Epoch: 6/100, iter: 2000. Loss (train/val): 0.312 / 0.297. Val. acc: 92.3%, Val. checks: 1/15\n",
      "Epoch finished. Elapsed time 4.0513 [s]\n",
      "\n",
      "Epoch: 7/100, iter: 2100. Loss (train/val): 0.267 / 0.247. Val. acc: 93.6%, Val. checks: 0/15\n",
      "Epoch: 7/100, iter: 2200. Loss (train/val): 0.380 / 0.312. Val. acc: 90.1%, Val. checks: 1/15\n",
      "Epoch: 7/100, iter: 2300. Loss (train/val): 0.291 / 0.295. Val. acc: 92.3%, Val. checks: 2/15\n",
      "Epoch finished. Elapsed time 4.6421 [s]\n",
      "\n",
      "Epoch: 8/100, iter: 2400. Loss (train/val): 0.272 / 0.275. Val. acc: 91.3%, Val. checks: 3/15\n",
      "Epoch: 8/100, iter: 2500. Loss (train/val): 0.570 / 0.579. Val. acc: 50.0%, Val. checks: 4/15\n",
      "Epoch: 8/100, iter: 2600. Loss (train/val): 0.266 / 0.247. Val. acc: 93.1%, Val. checks: 5/15\n",
      "Epoch finished. Elapsed time 5.2366 [s]\n",
      "\n",
      "Epoch: 9/100, iter: 2700. Loss (train/val): 0.243 / 0.200. Val. acc: 94.8%, Val. checks: 0/15\n",
      "Epoch: 9/100, iter: 2800. Loss (train/val): 0.236 / 0.187. Val. acc: 94.7%, Val. checks: 0/15\n",
      "Epoch: 9/100, iter: 2900. Loss (train/val): 0.232 / 0.205. Val. acc: 94.6%, Val. checks: 1/15\n",
      "Epoch: 9/100, iter: 3000. Loss (train/val): 0.268 / 0.245. Val. acc: 93.1%, Val. checks: 2/15\n",
      "Epoch finished. Elapsed time 5.8717 [s]\n",
      "\n",
      "Epoch: 10/100, iter: 3100. Loss (train/val): 0.243 / 0.216. Val. acc: 94.2%, Val. checks: 3/15\n",
      "Epoch: 10/100, iter: 3200. Loss (train/val): 0.539 / 0.520. Val. acc: 93.6%, Val. checks: 4/15\n",
      "Epoch: 10/100, iter: 3300. Loss (train/val): 0.477 / 0.465. Val. acc: 94.1%, Val. checks: 5/15\n",
      "Epoch finished. Elapsed time 6.4840 [s]\n",
      "\n",
      "Epoch: 11/100, iter: 3400. Loss (train/val): 0.329 / 0.315. Val. acc: 85.7%, Val. checks: 6/15\n",
      "Epoch: 11/100, iter: 3500. Loss (train/val): 0.207 / 0.215. Val. acc: 94.1%, Val. checks: 7/15\n",
      "Epoch: 11/100, iter: 3600. Loss (train/val): 0.234 / 0.228. Val. acc: 90.4%, Val. checks: 8/15\n",
      "Epoch finished. Elapsed time 7.0836 [s]\n",
      "\n",
      "Epoch: 12/100, iter: 3700. Loss (train/val): 0.221 / 0.204. Val. acc: 89.7%, Val. checks: 9/15\n",
      "Epoch: 12/100, iter: 3800. Loss (train/val): 0.248 / 0.254. Val. acc: 93.8%, Val. checks: 10/15\n",
      "Epoch: 12/100, iter: 3900. Loss (train/val): 0.335 / 0.361. Val. acc: 86.7%, Val. checks: 11/15\n",
      "Epoch: 12/100, iter: 4000. Loss (train/val): 0.204 / 0.221. Val. acc: 94.0%, Val. checks: 12/15\n",
      "Epoch finished. Elapsed time 7.7117 [s]\n",
      "\n",
      "Epoch: 13/100, iter: 4100. Loss (train/val): 0.317 / 0.287. Val. acc: 93.6%, Val. checks: 13/15\n",
      "Epoch: 13/100, iter: 4200. Loss (train/val): 0.182 / 0.166. Val. acc: 94.7%, Val. checks: 0/15\n",
      "Epoch: 13/100, iter: 4300. Loss (train/val): 0.173 / 0.189. Val. acc: 94.1%, Val. checks: 1/15\n",
      "Epoch finished. Elapsed time 8.3212 [s]\n",
      "\n",
      "Epoch: 14/100, iter: 4400. Loss (train/val): 0.153 / 0.158. Val. acc: 95.3%, Val. checks: 0/15\n",
      "Epoch: 14/100, iter: 4500. Loss (train/val): 0.181 / 0.181. Val. acc: 93.7%, Val. checks: 1/15\n",
      "Epoch: 14/100, iter: 4600. Loss (train/val): 0.176 / 0.195. Val. acc: 94.1%, Val. checks: 2/15\n",
      "Epoch finished. Elapsed time 8.9118 [s]\n",
      "\n",
      "Epoch: 15/100, iter: 4700. Loss (train/val): 0.164 / 0.172. Val. acc: 94.6%, Val. checks: 3/15\n",
      "Epoch: 15/100, iter: 4800. Loss (train/val): 0.156 / 0.194. Val. acc: 94.0%, Val. checks: 4/15\n",
      "Epoch: 15/100, iter: 4900. Loss (train/val): 0.173 / 0.201. Val. acc: 92.7%, Val. checks: 5/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/100, iter: 5000. Loss (train/val): 0.159 / 0.191. Val. acc: 92.8%, Val. checks: 6/15\n",
      "Epoch finished. Elapsed time 9.5486 [s]\n",
      "\n",
      "Epoch: 16/100, iter: 5100. Loss (train/val): 0.150 / 0.182. Val. acc: 95.3%, Val. checks: 7/15\n",
      "Epoch: 16/100, iter: 5200. Loss (train/val): 0.211 / 0.240. Val. acc: 89.8%, Val. checks: 8/15\n",
      "Epoch: 16/100, iter: 5300. Loss (train/val): 0.134 / 0.131. Val. acc: 96.6%, Val. checks: 0/15\n",
      "Epoch finished. Elapsed time 10.1612 [s]\n",
      "\n",
      "Epoch: 17/100, iter: 5400. Loss (train/val): 0.203 / 0.230. Val. acc: 91.4%, Val. checks: 1/15\n",
      "Epoch: 17/100, iter: 5500. Loss (train/val): 0.140 / 0.146. Val. acc: 95.7%, Val. checks: 2/15\n",
      "Epoch: 17/100, iter: 5600. Loss (train/val): 0.178 / 0.201. Val. acc: 94.4%, Val. checks: 3/15\n",
      "Epoch finished. Elapsed time 10.7670 [s]\n",
      "\n",
      "Epoch: 18/100, iter: 5700. Loss (train/val): 0.179 / 0.158. Val. acc: 96.1%, Val. checks: 4/15\n",
      "Epoch: 18/100, iter: 5800. Loss (train/val): 0.160 / 0.203. Val. acc: 94.2%, Val. checks: 5/15\n",
      "Epoch: 18/100, iter: 5900. Loss (train/val): 0.172 / 0.170. Val. acc: 95.9%, Val. checks: 6/15\n",
      "Epoch: 18/100, iter: 6000. Loss (train/val): 0.174 / 0.208. Val. acc: 94.6%, Val. checks: 7/15\n",
      "Epoch finished. Elapsed time 11.4109 [s]\n",
      "\n",
      "Epoch: 19/100, iter: 6100. Loss (train/val): 0.156 / 0.159. Val. acc: 95.7%, Val. checks: 8/15\n",
      "Epoch: 19/100, iter: 6200. Loss (train/val): 0.169 / 0.173. Val. acc: 93.2%, Val. checks: 9/15\n",
      "Epoch: 19/100, iter: 6300. Loss (train/val): 0.140 / 0.180. Val. acc: 94.9%, Val. checks: 10/15\n",
      "Epoch finished. Elapsed time 12.0022 [s]\n",
      "\n",
      "Epoch: 20/100, iter: 6400. Loss (train/val): 0.154 / 0.178. Val. acc: 95.3%, Val. checks: 11/15\n",
      "Epoch: 20/100, iter: 6500. Loss (train/val): 0.131 / 0.142. Val. acc: 95.3%, Val. checks: 12/15\n",
      "Epoch: 20/100, iter: 6600. Loss (train/val): 0.141 / 0.175. Val. acc: 95.2%, Val. checks: 13/15\n",
      "Epoch finished. Elapsed time 12.6388 [s]\n",
      "\n",
      "Epoch: 21/100, iter: 6700. Loss (train/val): 0.240 / 0.234. Val. acc: 96.1%, Val. checks: 14/15\n",
      "Epoch: 21/100, iter: 6800. Loss (train/val): 0.131 / 0.162. Val. acc: 95.4%, Val. checks: 15/15\n",
      "Early stopping\n",
      "Epoch finished. Elapsed time 12.9103 [s]\n",
      "\n",
      "\n",
      "\n",
      "[Beginning training of MLP at logdir \"./tarea_1_logs/experiment_14/run_4\"]\n",
      "\n",
      "Epoch: 1/100, iter: 0. Loss (train/val): 21.412 / 21.425. Val. acc: 50.0%, Val. checks: 0/15\n",
      "Epoch: 1/100, iter: 100. Loss (train/val): 0.688 / 0.689. Val. acc: 55.1%, Val. checks: 0/15\n",
      "Epoch: 1/100, iter: 200. Loss (train/val): 1.801 / 1.706. Val. acc: 64.0%, Val. checks: 1/15\n",
      "Epoch: 1/100, iter: 300. Loss (train/val): 1.696 / 1.524. Val. acc: 79.0%, Val. checks: 2/15\n",
      "Epoch finished. Elapsed time 0.9677 [s]\n",
      "\n",
      "Epoch: 2/100, iter: 400. Loss (train/val): 0.794 / 0.768. Val. acc: 50.0%, Val. checks: 3/15\n",
      "Epoch: 2/100, iter: 500. Loss (train/val): 0.688 / 0.640. Val. acc: 80.3%, Val. checks: 0/15\n",
      "Epoch: 2/100, iter: 600. Loss (train/val): 0.842 / 0.788. Val. acc: 50.0%, Val. checks: 1/15\n",
      "Epoch finished. Elapsed time 1.5635 [s]\n",
      "\n",
      "Epoch: 3/100, iter: 700. Loss (train/val): 0.908 / 0.858. Val. acc: 50.0%, Val. checks: 2/15\n",
      "Epoch: 3/100, iter: 800. Loss (train/val): 0.751 / 0.722. Val. acc: 50.0%, Val. checks: 3/15\n",
      "Epoch: 3/100, iter: 900. Loss (train/val): 0.906 / 0.759. Val. acc: 84.9%, Val. checks: 4/15\n",
      "Epoch: 3/100, iter: 1000. Loss (train/val): 1.191 / 1.098. Val. acc: 50.0%, Val. checks: 5/15\n",
      "Epoch finished. Elapsed time 2.1960 [s]\n",
      "\n",
      "Epoch: 4/100, iter: 1100. Loss (train/val): 0.686 / 0.601. Val. acc: 86.5%, Val. checks: 0/15\n",
      "Epoch: 4/100, iter: 1200. Loss (train/val): 0.448 / 0.416. Val. acc: 87.6%, Val. checks: 0/15\n",
      "Epoch: 4/100, iter: 1300. Loss (train/val): 0.400 / 0.360. Val. acc: 88.4%, Val. checks: 0/15\n",
      "Epoch finished. Elapsed time 2.7916 [s]\n",
      "\n",
      "Epoch: 5/100, iter: 1400. Loss (train/val): 0.447 / 0.429. Val. acc: 87.0%, Val. checks: 1/15\n",
      "Epoch: 5/100, iter: 1500. Loss (train/val): 0.368 / 0.344. Val. acc: 89.6%, Val. checks: 0/15\n",
      "Epoch: 5/100, iter: 1600. Loss (train/val): 1.270 / 1.243. Val. acc: 78.3%, Val. checks: 1/15\n",
      "Epoch finished. Elapsed time 3.3864 [s]\n",
      "\n",
      "Epoch: 6/100, iter: 1700. Loss (train/val): 0.333 / 0.322. Val. acc: 90.0%, Val. checks: 0/15\n",
      "Epoch: 6/100, iter: 1800. Loss (train/val): 0.317 / 0.285. Val. acc: 91.9%, Val. checks: 0/15\n",
      "Epoch: 6/100, iter: 1900. Loss (train/val): 0.513 / 0.476. Val. acc: 90.1%, Val. checks: 1/15\n",
      "Epoch: 6/100, iter: 2000. Loss (train/val): 0.317 / 0.303. Val. acc: 91.7%, Val. checks: 2/15\n",
      "Epoch finished. Elapsed time 4.0207 [s]\n",
      "\n",
      "Epoch: 7/100, iter: 2100. Loss (train/val): 0.356 / 0.323. Val. acc: 92.7%, Val. checks: 3/15\n",
      "Epoch: 7/100, iter: 2200. Loss (train/val): 0.293 / 0.291. Val. acc: 92.4%, Val. checks: 4/15\n",
      "Epoch: 7/100, iter: 2300. Loss (train/val): 0.490 / 0.499. Val. acc: 90.7%, Val. checks: 5/15\n",
      "Epoch finished. Elapsed time 4.6176 [s]\n",
      "\n",
      "Epoch: 8/100, iter: 2400. Loss (train/val): 0.382 / 0.361. Val. acc: 88.5%, Val. checks: 6/15\n",
      "Epoch: 8/100, iter: 2500. Loss (train/val): 0.283 / 0.284. Val. acc: 91.9%, Val. checks: 0/15\n",
      "Epoch: 8/100, iter: 2600. Loss (train/val): 0.247 / 0.244. Val. acc: 93.4%, Val. checks: 0/15\n",
      "Epoch finished. Elapsed time 5.2096 [s]\n",
      "\n",
      "Epoch: 9/100, iter: 2700. Loss (train/val): 0.283 / 0.309. Val. acc: 91.6%, Val. checks: 1/15\n",
      "Epoch: 9/100, iter: 2800. Loss (train/val): 0.243 / 0.262. Val. acc: 93.0%, Val. checks: 2/15\n",
      "Epoch: 9/100, iter: 2900. Loss (train/val): 0.308 / 0.303. Val. acc: 91.4%, Val. checks: 3/15\n",
      "Epoch: 9/100, iter: 3000. Loss (train/val): 0.209 / 0.227. Val. acc: 94.0%, Val. checks: 0/15\n",
      "Epoch finished. Elapsed time 5.8406 [s]\n",
      "\n",
      "Epoch: 10/100, iter: 3100. Loss (train/val): 0.313 / 0.259. Val. acc: 94.1%, Val. checks: 1/15\n",
      "Epoch: 10/100, iter: 3200. Loss (train/val): 0.220 / 0.209. Val. acc: 94.6%, Val. checks: 0/15\n",
      "Epoch: 10/100, iter: 3300. Loss (train/val): 0.222 / 0.244. Val. acc: 94.1%, Val. checks: 1/15\n",
      "Epoch finished. Elapsed time 6.4286 [s]\n",
      "\n",
      "Epoch: 11/100, iter: 3400. Loss (train/val): 0.225 / 0.255. Val. acc: 93.0%, Val. checks: 2/15\n",
      "Epoch: 11/100, iter: 3500. Loss (train/val): 0.221 / 0.244. Val. acc: 92.5%, Val. checks: 3/15\n",
      "Epoch: 11/100, iter: 3600. Loss (train/val): 0.322 / 0.327. Val. acc: 94.8%, Val. checks: 4/15\n",
      "Epoch finished. Elapsed time 7.0206 [s]\n",
      "\n",
      "Epoch: 12/100, iter: 3700. Loss (train/val): 0.211 / 0.230. Val. acc: 94.1%, Val. checks: 5/15\n",
      "Epoch: 12/100, iter: 3800. Loss (train/val): 0.212 / 0.248. Val. acc: 93.1%, Val. checks: 6/15\n",
      "Epoch: 12/100, iter: 3900. Loss (train/val): 0.329 / 0.315. Val. acc: 94.2%, Val. checks: 7/15\n",
      "Epoch: 12/100, iter: 4000. Loss (train/val): 0.353 / 0.302. Val. acc: 94.2%, Val. checks: 8/15\n",
      "Epoch finished. Elapsed time 7.6605 [s]\n",
      "\n",
      "Epoch: 13/100, iter: 4100. "
     ]
    }
   ],
   "source": [
    "run_n_times = 5\n",
    "stats_history = []\n",
    "for run in range(run_n_times):\n",
    "    # ----- Creacion de MLP\n",
    "    mlp = MLPClassifier(\n",
    "        n_features=28*28,\n",
    "        layer_sizes=[25, 2],\n",
    "        loss_function_name='cross_entropy',\n",
    "        learning_rate=10,\n",
    "        batch_size=32,\n",
    "        max_epochs=100,\n",
    "        early_stopping=15,\n",
    "        logdir=logdir+'/run_%d' % run)\n",
    "\n",
    "    # ----- Entrenamiento de MLP\n",
    "    train_stats = mlp.fit(training_images, training_labels, validation_images, validation_labels)\n",
    "    stats_history.append(train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uszFkplq3avX"
   },
   "source": [
    "### Tasa de acierto en validación computada sobre varias ejecuciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "HNS4VAkflQvE",
    "outputId": "51adcf4c-1285-4a67-c66b-ac27a5d77a58"
   },
   "outputs": [],
   "source": [
    "val_acc_history = np.array([run['val_acc_history'][-1] for run in stats_history])\n",
    "val_acc_mean = val_acc_history.mean()\n",
    "val_acc_std = val_acc_history.std()\n",
    "print('Validation accuracy %.3f +/- %.3f' % (val_acc_mean, val_acc_std))\n",
    "print(val_acc_history)\n",
    "\n",
    "train_acc_history = np.array([run['train_acc_history'][-1] for run in stats_history])\n",
    "train_acc_mean = train_acc_history.mean()\n",
    "train_acc_std = train_acc_history.std()\n",
    "print('Train accuracy %.3f +/- %.3f' % (train_acc_mean, train_acc_std))\n",
    "print(train_acc_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tDrpTQWXa19D"
   },
   "source": [
    "## Visualización de Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ypv7zhabDJT"
   },
   "source": [
    "### Algunas estadísticas del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "XsDDF3BoboJS",
    "outputId": "d8d6b2bc-0e16-4fb8-8e5a-1cf966c89532"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "ax[0].plot(train_stats['iteration_history'], train_stats['val_loss_history'], label='validation')\n",
    "ax[0].plot(train_stats['iteration_history'], train_stats['train_loss_history'], label='training')\n",
    "ax[0].set_xlabel('Iteration')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Loss evolution during training')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(train_stats['iteration_history'], train_stats['val_acc_history'], label='validation')\n",
    "ax[1].plot(train_stats['iteration_history'], train_stats['train_acc_history'], label='training')\n",
    "ax[1].set_xlabel('Iteration')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].set_title('Accuracy evolution during training')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PF87rmUxbMtW"
   },
   "source": [
    "### Estadísticas del desempeño final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XKM3ashMUVZa",
    "outputId": "1c7647c6-f86b-41d3-8fc8-2d3c866312d8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(labels, predictions):\n",
    "    \"\"\"Calcula la matriz de confusion.\n",
    "    \n",
    "    Args:\n",
    "        labels: Array binario 1-D con las etiquetas reales.\n",
    "        predictions: Array binario 1-D con las predicciones.\n",
    "        \n",
    "    Returns:\n",
    "        TP: Numero de verdaderos positivos.\n",
    "        FP: Numero de falsos positivos.\n",
    "        FN: Numero de falsos negativos.\n",
    "        TN: Numero de verdaderos negativos.\n",
    "    \"\"\"\n",
    "    # Map labels and predictions to {0, 1, 2, 3}\n",
    "    encoded_data = 2 * labels + predictions  \n",
    "    TN = np.sum(encoded_data == 0)  # True negatives\n",
    "    FP = np.sum(encoded_data == 1)  # False positives\n",
    "    FN = np.sum(encoded_data == 2)  # False negatives\n",
    "    TP = np.sum(encoded_data == 3)  # True positives\n",
    "    return TP, FP, FN, TN\n",
    "\n",
    "\n",
    "def performance_metrics(TP, FP, FN, TN):\n",
    "    \"\"\"Calcula metricas de desempeño.\n",
    "    \n",
    "    Args:\n",
    "        TP: Numero de verdaderos positivos.\n",
    "        FP: Numero de falsos positivos.\n",
    "        FN: Numero de falsos negativos.\n",
    "        TN: Numero de verdaderos negativos.\n",
    "    \n",
    "    Returns:\n",
    "        accuracy: Porcentaje de clasificaciones correctas del detector.\n",
    "        precision: Precision del detector.\n",
    "        recall: Recall/Sensibilidad del detector.\n",
    "    \"\"\"\n",
    "    accuracy = 100.0 * (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = 100.0 * TP / (TP + FP)\n",
    "    recall = 100.0 * TP / (TP + FN)\n",
    "    print('TP: %d, TN: %d, FP: %d, FN: %d' %(TP,TN,FP,FN))\n",
    "    print('%1.4f%% Accuracy (Porcentaje de clasificaciones correctas)' % (accuracy))\n",
    "    print('%1.4f%% Precision' % (precision))\n",
    "    print('%1.4f%% Recall' % (recall))\n",
    "    print('')\n",
    "    return accuracy, precision, recall\n",
    "\n",
    "\n",
    "def roc_curve(labels, probabilities):\n",
    "    \"\"\"Calcula la curva ROC.\n",
    "    \n",
    "    Args:\n",
    "        labels: Array binario 1-D con las etiquetas reales.\n",
    "        probabilities: Array 1-D continuo en el rango [0, 1] con las\n",
    "            probabilidades de la clase 1.\n",
    "        \n",
    "    Returns:\n",
    "        tpr: Array 1-D con los valores de Tasa de Verdaderos Positivos (TPR).\n",
    "        fpr: Array 1-D con los valores de Tasa de Falsos Positivos (FPR).\n",
    "    \"\"\"\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for threshold in np.linspace(0, 1, 1000):\n",
    "        probabilities_with_threshold = (probabilities > threshold).astype(np.float)\n",
    "        TP, FP, FN, TN = confusion_matrix(\n",
    "            labels, \n",
    "            probabilities_with_threshold)\n",
    "        tpr.append(TP/(TP+FN))\n",
    "        fpr.append(FP/(FP+TN))\n",
    "    return np.array(tpr), np.array(fpr)\n",
    "  \n",
    "\n",
    "def detection_performance_given_threshold(true_labels, prediction, threshold):\n",
    "    probabilities_with_threshold = (prediction > threshold).astype(np.float)\n",
    "    TP, FP, FN, TN = confusion_matrix(\n",
    "        true_labels, \n",
    "        probabilities_with_threshold)    \n",
    "    return TP, FP, FN, TN       \n",
    "\n",
    "  \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "predicted_train_labels = mlp.predict_label(training_images)\n",
    "predicted_val_labels = mlp.predict_label(validation_images)\n",
    "predicted_test_labels = mlp.predict_label(testing_images)\n",
    "\n",
    "cnf_matrix = sk_conf_mat(testing_labels, predicted_test_labels)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plt.grid('off')\n",
    "plt.gcf().set_facecolor('white')\n",
    "plot_confusion_matrix(cnf_matrix, classes=['%d' % chosen_digit, 'no %d' % chosen_digit],\n",
    "                      title='Confusion matrix')\n",
    "\n",
    "\n",
    "print('Training results:')\n",
    "TP, FP, FN, TN = confusion_matrix(training_labels, predicted_train_labels)\n",
    "accuracy, precision, recall = performance_metrics(TP, FP, FN, TN)\n",
    "\n",
    "print('Validation results:')\n",
    "TP, FP, FN, TN = confusion_matrix(validation_labels, predicted_val_labels)\n",
    "accuracy, precision, recall = performance_metrics(TP, FP, FN, TN)\n",
    "\n",
    "print('Test results:')\n",
    "TP, FP, FN, TN = confusion_matrix(testing_labels, predicted_test_labels)\n",
    "accuracy, precision, recall = performance_metrics(TP, FP, FN, TN)\n",
    "\n",
    "predicted_test_proba = mlp.predict_proba(testing_images)\n",
    "tpr, fpr = roc_curve(testing_labels, predicted_test_proba[:, 1])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "ax[0].set_title('ROC Curve')\n",
    "ax[0].plot(fpr, tpr, linewidth=2, alpha=0.5)\n",
    "ax[0].set_xlabel('False Positive Rate')\n",
    "ax[0].set_ylabel('True Positive Rate')\n",
    "\n",
    "ax[1].set_title('DET Curve')\n",
    "ax[1].plot(fpr, 1.0 - tpr, linewidth=2, alpha=0.5)\n",
    "ax[1].set_xlabel('False Positive Rate')\n",
    "ax[1].set_ylabel('False Negative Rate')\n",
    "ax[1].set_yscale('log')\n",
    "ax[1].set_xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lsWfO5koe6wg"
   },
   "source": [
    "### Performance según umbral de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "neruAyQbecmn",
    "outputId": "ce4a4256-5e2f-457a-ea28-48abc498fab7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_validation_proba = mlp.predict_proba(validation_images)\n",
    "TP, FP, FN, TN = detection_performance_given_threshold(validation_labels, predicted_validation_proba[:, 1], threshold=0.6)\n",
    "print('TP: %d, TN: %d, FP: %d, FN: %d' %(TP,TN,FP,FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X2zFsvMobYvZ"
   },
   "source": [
    "### Visualización de clasificaciones en el test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 810
    },
    "colab_type": "code",
    "id": "KyzPqoEgV0ky",
    "outputId": "236b21ed-b683-4aac-ff31-b7d0ffd2ff71"
   },
   "outputs": [],
   "source": [
    "def show_classifications(images, labels, probabilities, result_type='TP'):\n",
    "    \"\"\" Muestra ejemplos de imagenes para tipos de errores.\n",
    "    \n",
    "    Args:\n",
    "        images: Array de dimensiones (n_ejemplos, n_pixeles) con imagenes.\n",
    "        labels: Array de dimensiones (n_ejemplos,) con las etiquetas reales.\n",
    "        probabilities: Array de dimensiones (n_ejemplos,) con las probabilidades\n",
    "            de la clase 1.\n",
    "        result_type: 'TP', 'FP', 'FN', o 'TP', tipo de error a mostrar.\n",
    "    \"\"\"\n",
    "    dict_types = {'TN': 0, 'FP': 1, 'FN': 2, 'TP': 3}\n",
    "    predictions = (probabilities > 0.5).astype(np.int32)\n",
    "    encoded_data = 2 * labels + predictions \n",
    "    useful = np.where(encoded_data == dict_types[result_type])[0]\n",
    "    size = min(4, useful.shape[0])\n",
    "    chosen = np.random.choice(useful, size=size, replace=False)\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(10,4))\n",
    "    fig.set_facecolor('white')\n",
    "    for i, idx in enumerate(chosen):\n",
    "        image = images[idx, :]\n",
    "        digit = labels[idx]\n",
    "        predicted_label = predictions[idx]\n",
    "        proba = probabilities[idx]\n",
    "        ax[i].imshow(image.reshape((28, 28)))\n",
    "        ax[i].set_title(\"True Class: %d\\nPredicted Class: %d\\nProb. of Class 1: %1.4f\"\n",
    "                        % (digit, predicted_label, proba))\n",
    "        ax[i].axis('off')\n",
    "    for j in range(i+1, 4):\n",
    "        ax[j].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "predicted_test_proba = mlp.predict_proba(testing_images)\n",
    "predicted_test_proba = predicted_test_proba[:, 1]\n",
    "\n",
    "print('True Positives:')\n",
    "show_classifications(testing_images, testing_labels, predicted_test_proba , result_type='TP')\n",
    "\n",
    "print('True Negatives:')\n",
    "show_classifications(testing_images, testing_labels, predicted_test_proba , result_type='TN')\n",
    "\n",
    "print('False Positive:')\n",
    "show_classifications(testing_images, testing_labels, predicted_test_proba , result_type='FP')\n",
    "\n",
    "print('False Negative:')\n",
    "show_classifications(testing_images, testing_labels, predicted_test_proba , result_type='FN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7b6uAwj7q2TA"
   },
   "source": [
    "Si quieren borrar su log_dir, utilicen lo siguiente:\n",
    "\n",
    "Ejecutar comandos en bash\n",
    "* %%bash\n",
    "\n",
    "para mirar lo que hay en el directorio actual:\n",
    "* ls \n",
    "\n",
    "para borrar la carpeta \"tarea_1_logs\"\n",
    "* rm -r tarea_1_logs\n",
    "\n",
    "buscar procesos asociados a tensorboard\n",
    "* ps aux | grep tensorboard\n",
    "\n",
    "terminar un proceso\n",
    "* kill -9 process_id (process_id es el pid del proceso)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "WDbgq2lP4xZy",
    "outputId": "01b03a30-f2f8-4e00-9a60-0b008a1d4383"
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# ls tarea_1_logs/exp_xentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "veKtB6KRkLIu"
   },
   "outputs": [],
   "source": [
    "#!rm -r tarea_1_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JviRuQufPeZf",
    "outputId": "9c280763-b2af-420a-bea0-5d640a86a54d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q4cnGTMyKsdW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Tarea1_Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
